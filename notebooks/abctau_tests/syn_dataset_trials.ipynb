{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using abcTau to fit ACFs for trials (Figure 2 from the paper). \n",
    "\n",
    "Three options to do that:\n",
    "* use abcTau package for both ACF and fitting\n",
    "* use ACF calculated before using acf function\n",
    "* use ACF calculated before using iSTTC concat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# add the path to the abcTau package\n",
    "import sys\n",
    "#sys.path.append('./abcTau')\n",
    "sys.path.append('C:\\\\Users\\\\ipochino\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\isttc\\\\Lib\\\\site-packages\\\\abcTau') # IP: replaced previous line with that; relative path was not working\n",
    "import abcTau\n",
    "\n",
    "from isttc.scripts.cfg_global import project_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = project_folder_path + 'synthetic_dataset\\\\'\n",
    "results_folder = project_folder_path + 'results\\\\synthetic\\\\results\\\\param_fr_alpha_tau\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data (spike trains and calculated acf's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n spike trains 100000, trial_lens 1000 ms\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_folder + 'trials40_binned.pkl','rb') as f:\n",
    "    data_binned = pickle.load(f)\n",
    "\n",
    "trial_dict_binned = data_binned['trial_dict']\n",
    "alphas_binned  = data_binned['alphas']\n",
    "fr_values_binned  = data_binned['fr_values']\n",
    "taus_ms_binned  = data_binned['tau_ms']\n",
    "n_trials_binned  = data_binned['n_trials']\n",
    "trial_lens_binned  = data_binned['trial_lens']\n",
    "\n",
    "print(f'n spike trains {len(trial_dict_binned)}, trial_lens {trial_lens_binned[0]} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### abcTau for both ACF and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parameters\n",
    "summStat_metric = 'comp_cc'\n",
    "ifNorm = True # if normalize the autocorrelation or PSD\n",
    "deltaT = 1 # temporal resolution of data.\n",
    "binSize = 50 #  bin-size for binning data and computing the autocorrelation.\n",
    "disp = None # put the dispersion parameter if computed with grid-search\n",
    "maxTimeLag = 1000 # only used when using autocorrelation for summary statistics\n",
    "#lm = round(maxTimeLag/binSize) # maximum bin for autocorrelation computation\n",
    "\n",
    "# desired generative model from the list of 'generative_models.py' and the distance function from 'diatance_functions.py'\n",
    "generativeModel = 'oneTauOU_gammaSpikes'\n",
    "distFunc = 'linear_distance'\n",
    "\n",
    "# Define a uniform prior distribution over the given range\n",
    "# for a uniform prior: stats.uniform(loc=x_min,scale=x_max-x_min)\n",
    "t_min = 0.0 # first timescale\n",
    "t_max = 400.0\n",
    "priorDist = [stats.uniform(loc= t_min, scale = t_max - t_min)]\n",
    "\n",
    "# aABC fitting parameters\n",
    "epsilon_0 = 1  # initial error threshold\n",
    "min_samples = 50 # min samples from the posterior\n",
    "steps = 30 # max number of iterations\n",
    "minAccRate = 0.01 # minimum acceptance rate to stop the iterations\n",
    "parallel = False # if parallel processing\n",
    "n_procs = 1 # number of processor for parallel processing (set to 1 if there is no parallel processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 0\n",
    "\n",
    "# spike_binned = trial_dict_binned[idx][0]\n",
    "# numTrials = n_trials_binned[idx]\n",
    "# T = trial_lens_binned[idx]\n",
    "\n",
    "# numBinData = spike_binned.shape[1]\n",
    "# data_mean = np.mean(spike_binned)\n",
    "# data_var = abcTau.preprocessing.comp_cc(spike_binned, spike_binned, 1, binSize, numBinData)[0]\n",
    "# # population variance within each trial, then average across trials - same as comp_cc\n",
    "# # data_var_lib = np.mean(np.var(spike_binned, axis=1, ddof=0))\n",
    "# disp = data_var / data_mean\n",
    "\n",
    "# data_sumStat = abcTau.summary_stats.comp_sumStat(spike_binned, summStat_metric, ifNorm, deltaT, binSize, T, numBinData, maxTimeLag)\n",
    "\n",
    "# print(f'sumStat len {data_sumStat.shape}, data_mean {data_mean}, data_var {data_var}, disp {disp}, T {T}, numTrials {numTrials}, numBinData {numBinData}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(abcTau.Model):\n",
    "\n",
    "    #This method initializes the model object.  \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # draw samples from the prior. \n",
    "    def draw_theta(self):\n",
    "        theta = []\n",
    "        for p in self.prior:\n",
    "            theta.append(p.rvs())\n",
    "        return theta\n",
    "\n",
    "    # Choose the generative model (from generative_models)\n",
    "    # Choose autocorrelation computation method (from basic_functions)\n",
    "    def generate_data(self, theta):\n",
    "        #print(f'DISP {disp}')\n",
    "        # generate synthetic data\n",
    "        if disp == None:\n",
    "            syn_data, numBinData =  eval('abcTau.generative_models.' + generativeModel + \\\n",
    "                                         '(theta, deltaT, binSize, T, numTrials, data_mean, data_var)')\n",
    "        else:\n",
    "            syn_data, numBinData =  eval('abcTau.generative_models.' + generativeModel + \\\n",
    "                                         '(theta, deltaT, binSize, T, numTrials, data_mean, data_var, disp)')\n",
    "               \n",
    "        # compute the summary statistics\n",
    "        syn_sumStat = abcTau.summary_stats.comp_sumStat(syn_data, summStat_metric, ifNorm, deltaT, binSize, T,\\\n",
    "                                          numBinData, maxTimeLag)   \n",
    "        # print(f'syn_sumStat len {syn_sumStat.shape}, syn_sumStat {syn_sumStat}, numBinData {numBinData}, \\\n",
    "        # deltaT {deltaT}, binSize {binSize}, T {T}, numTrials {numTrials}, data_mean {data_mean}, data_var {data_var}')\n",
    "        \n",
    "        return syn_sumStat\n",
    "\n",
    "    # Computes the summary statistics\n",
    "    def summary_stats(self, data):\n",
    "        sum_stat = data\n",
    "        return sum_stat\n",
    "\n",
    "    # Choose the method for computing distance (from basic_functions)\n",
    "    def distance_function(self, data, synth_data):\n",
    "        if np.nansum(synth_data) <= 0: # in case of all nans return large d to reject the sample\n",
    "            d = 10**4\n",
    "        else:\n",
    "            d = eval('abcTau.distance_functions.' +distFunc + '(data, synth_data)')        \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder_abctau = project_folder_path + 'results\\\\synthetic\\\\results\\\\param_fr_alpha_tau_abctau\\\\test_timeout\\\\'\n",
    "# path and filename to save the intermediate results after running each step\n",
    "inter_save_direc = results_folder_abctau + 'interim_results\\\\'\n",
    "datasave_path = results_folder_abctau + 'final_results\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in list(trial_dict_binned.items())[33:34]:\n",
    "    spike_binned = v[0]\n",
    "    numTrials = n_trials_binned[k]\n",
    "    T = trial_lens_binned[k]\n",
    "\n",
    "    numBinData = spike_binned.shape[1]\n",
    "    data_mean = np.mean(spike_binned)\n",
    "    data_var = abcTau.preprocessing.comp_cc(spike_binned, spike_binned, 1, binSize, numBinData)[0]\n",
    "    data_var = abcTau.preprocessing.comp_cc(spike_binned, spike_binned, 1, binSize, numBinData)[0]\n",
    "    # population variance within each trial, then average across trials - same as comp_cc\n",
    "    # data_var_lib = np.mean(np.var(spike_binned, axis=1, ddof=0))\n",
    "    disp = data_var / data_mean\n",
    "    \n",
    "    data_sumStat = abcTau.summary_stats.comp_sumStat(spike_binned, summStat_metric, ifNorm, deltaT, binSize, T, numBinData, maxTimeLag)\n",
    "    print(f'sumStat len {data_sumStat.shape}, data_mean {data_mean}, data_var {data_var}, disp {disp}, T {T}, numTrials {numTrials}, numBinData {numBinData}')\n",
    "\n",
    "    # Run the aABC algorithm and save the results\n",
    "    filenameSave = 'spike_train_' + str(k)\n",
    "    inter_filename = 'spike_train_interim_' + str(k)\n",
    "    abc_results, final_step = abcTau.fit.fit_withABC(MyModel, data_sumStat, priorDist, inter_save_direc, inter_filename,\\\n",
    "                                                     datasave_path, filenameSave, epsilon_0, min_samples, \\\n",
    "                                                     steps, minAccRate, parallel, n_procs, disp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
