{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8233ff1-87a2-45ea-a94a-f7974747f389",
   "metadata": {},
   "source": [
    "Here:\n",
    "- generate time series (cont OU process?)\n",
    "- plot it (continious and also binned)\n",
    "- calculate acf: using ABC tau, using py functions\n",
    "- plot acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95934c7-5172-4158-8b13-fcb057908108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns # comment this line if you don't want to use seaborn for plots\n",
    "\n",
    "import numpy as np\n",
    "# IP import warnings because of \"module 'numpy' has no attribute 'warnings'\"\n",
    "import warnings\n",
    "np.warnings = warnings\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "# add the path to the abcTau package\n",
    "import sys\n",
    "#sys.path.append('./abcTau')\n",
    "sys.path.append('C:\\\\Users\\\\ipochino\\\\.conda\\\\envs\\\\isttc\\\\Lib\\\\site-packages\\\\abcTau') # IP: replaced previous line with that; relative path was not working\n",
    "import abcTau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ef09e-3cfd-4c32-b2fc-e47b9710e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OU_gen(tau, D, deltaT, T, numTrials):\n",
    "    \"\"\"Generate an OU process with a single timescale, zero mean and unit variance.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    tau : float\n",
    "        timescale.\n",
    "    D : float\n",
    "        diffusion parameter.\n",
    "    deltaT : float\n",
    "        temporal resolution for the OU process generation.    \n",
    "    T : float\n",
    "        duration of trials.\n",
    "    numTrials : float\n",
    "        number of trials.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ou : nd array\n",
    "        array of generated OU process (numTrials * (T/deltaT)).\n",
    "    \"\"\"\n",
    "    \n",
    "    numBin = int(T/deltaT)\n",
    "    noise =  np.random.normal(loc=0,scale=1, size=(numTrials,numBin))\n",
    "    ou = np.zeros((numTrials,numBin))\n",
    "    ou[:,0] = noise[:,0]\n",
    "    for iBin in range(1,numBin):\n",
    "        ou[:,iBin]  = ou[:,iBin-1] - (ou[:,iBin-1]/tau) * deltaT + np.sqrt(2*D*deltaT) * noise[:,iBin-1]\n",
    "        \n",
    "    return ou\n",
    "# IP comments:\n",
    "# 1. not sure why we multiply by deltaT in diffusion part - the finer the resolution (higher numBin) the less diffusion we have - this sort \n",
    "# of make sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048b6381-429d-4eee-9d28-89acac735d35",
   "metadata": {},
   "source": [
    "### Generate time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3190e170-88bb-42b7-b142-31b711eddbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = np.array([100])\n",
    "D = 1/tau\n",
    "deltaT = 1\n",
    "T = 1000\n",
    "numTrials = 1\n",
    "\n",
    "# generate OU\n",
    "#ou_all = OU_gen(tau, D, deltaT, T, numTrials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e50d53-f208-4014-9b45-5f46d593b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate OU\n",
    "numBin = int(T/deltaT)\n",
    "print('numBin:', numBin)\n",
    "\n",
    "noise =  np.random.normal(loc=0,scale=1, size=(numTrials,numBin))\n",
    "print('noise.shape: ', noise.shape)\n",
    "\n",
    "noise_ou = np.sqrt(2*D*deltaT) * noise\n",
    "print('noise_ou.shape: ', noise_ou.shape)\n",
    "\n",
    "ou = np.zeros((numTrials,numBin))\n",
    "ou[:,0] = noise[:,0]\n",
    "for iBin in range(1,numBin):\n",
    "    ou[:,iBin]  = ou[:,iBin-1] - (ou[:,iBin-1]/tau) * deltaT + np.sqrt(2*D*deltaT) * noise[:,iBin-1]\n",
    "\n",
    "print('ou.shape: ', ou.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251f742-508a-47bf-9cad-22331be1b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust OU process - scale, shift and rectify (to get time-varying rate)\n",
    "\n",
    "data_mean = 1 # average of firing rate\n",
    "data_var = 1.5 # variance of firing rate\n",
    "\n",
    "ou_check = np.max(ou)\n",
    "print(ou_check)\n",
    "\n",
    "ou_std = np.sqrt(data_var)\n",
    "ou_mean = data_mean\n",
    "ou_all = ou_std * ou + ou_mean # this is scale and shift, where is rectify?\n",
    "\n",
    "print('ou_all.shape: ', ou_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1aab85-732a-4b4f-bd6f-c0f84093d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "binSize = 50 # bins size for binning the data for calculating acf\n",
    "binsData =  np.arange(0, T + binSize, binSize)\n",
    "numBinData = len(binsData)-1\n",
    "print(binsData)\n",
    "print(numBinData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801f49b-349a-4c30-9d49-93b6b674bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_shape = [numTrials, numBinData]\n",
    "shape = (new_shape[0], ou_all.shape[0] // new_shape[0],\n",
    "         new_shape[1], ou_all.shape[1] // new_shape[1])\n",
    "print(shape)\n",
    "binned_data = ou_all.reshape(shape).sum(-1).sum(1)\n",
    "print(binned_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07e3413-c39c-478f-b670-7c2b3976e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_data = ou_all.reshape(shape).sum(-1).sum(1)\n",
    "binned_data.shape\n",
    "binned_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fff744-78be-4123-be12-198f2061fb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5,1, figsize=(10,8))\n",
    "\n",
    "axes[0].plot(np.linspace(0,numBin-1,numBin), noise[0], lw=0.5)\n",
    "axes[0].axhline(y=0, color='k', lw=0.5)\n",
    "\n",
    "axes[1].plot(np.linspace(0,numBin-1,numBin), noise_ou[0], lw=0.5)\n",
    "axes[1].axhline(y=0, color='k', lw=0.5)\n",
    "\n",
    "axes[2].plot(np.linspace(0,numBin-1,numBin), ou[0], lw=0.5)\n",
    "axes[2].axhline(y=0, color='k', lw=0.5)\n",
    "\n",
    "axes[3].plot(np.linspace(0,numBin-1,numBin), ou_all[0], lw=0.5)\n",
    "axes[3].axhline(y=0, color='k', lw=0.5)\n",
    "for i in range(numBinData+1):\n",
    "    axes[3].axvline(x=50*i, color='k', lw=0.5)\n",
    "\n",
    "# that is supposed to be binned rate but it goes negative? is it ok?\n",
    "axes[4].plot(np.linspace(0,numBinData-1,numBinData), binned_data[0], lw=0.5)\n",
    "axes[4].axhline(y=0, color='k', lw=0.5)\n",
    "axes[4].set_xticks(np.linspace(0,numBinData-1,numBinData))\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d512c-49bf-494d-b3eb-080104717c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.autocorrelation_plot(ou_all[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3950c41-46af-4f12-b7b6-52c6f3dea520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#binSize = 50\n",
    "maxTimeLag = 1000 \n",
    "summStat_metric = 'comp_cc'\n",
    "ifNorm = True # if normalize the autocorrelation or PSD\n",
    "data_sumStat, data_mean, data_var, T, numTrials =  abcTau.preprocessing.extract_stats(ou_all, deltaT, binSize,\n",
    "                                                                                  summStat_metric, ifNorm, maxTimeLag)\n",
    "print(data_sumStat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e21f9-26f6-42e8-8641-bd8678fb0a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_axis_lags = np.linspace(0, 1000, 20)\n",
    "\n",
    "fig, axes = plt.subplots(1,1, figsize=(7,5))\n",
    "\n",
    "axes.plot(t_axis_lags, data_sumStat, c='k', label='abc')\n",
    "axes.plot(t_axis_lags, acf_, c='orange', label='acf_statstool')\n",
    "pd.plotting.autocorrelation_plot(ou_all[0,:], ax=axes, c='steelblue', label='autocorr_plot')\n",
    "#pd.plotting.autocorrelation_plot(ou[0,:], ax=axes, c='darkblue', label='autocorr_plot')\n",
    "axes.axvline(x=100, lw=0.5, color='k')\n",
    "axes.axhline(y=1/np.e, lw=0.5, color='k')\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b146c2f1-67ce-4fe6-8560-1e650d050dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "acf_ = acf(binned_data[0], nlags=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b29dcb-8a17-4998-8b27-39d4f4946a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "popt, poptcov = abcTau.preprocessing.fit_oneTauExponential(data_sumStat, binSize, maxTimeLag)\n",
    "tau = popt[1]\n",
    "\n",
    "print(popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e347f5d-d8ac-4093-a5e3-c8b4ebba5d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "def func_exp(x, a, b, c):\n",
    "    \"\"\"\n",
    "    Exponential function to fit the data.\n",
    "    :param x: 1d array, independent variable\n",
    "    :param a: float, parameter to fit\n",
    "    :param b: float, parameter to fit\n",
    "    :param c: float, parameter to fit\n",
    "    :return: callable\n",
    "    \"\"\"\n",
    "    return a * np.exp(-b * x) + c\n",
    "    \n",
    "popt, pcov = curve_fit(func_exp, np.linspace(0,20,20), acf_[0:], maxfev=5000)\n",
    "print(popt)\n",
    "\n",
    "tau = 1 / popt[1]\n",
    "tau_ms = tau * binSize\n",
    "print(tau, tau_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090bc344-a4e3-4b9c-a4a5-c7d5a27449cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select generative model and distance function\n",
    "generativeModel = 'oneTauOU'\n",
    "distFunc = 'linear_distance'\n",
    "\n",
    "# Define a uniform prior distribution over the given range\n",
    "# for a uniform prior: stats.uniform(loc=x_min,scale=x_max-x_min)\n",
    "t_min = 0.0 # first timescale\n",
    "t_max = 100.0\n",
    "priorDist = [stats.uniform(loc= t_min, scale = t_max - t_min)]\n",
    "\n",
    "# set fitting params\n",
    "epsilon_0 = 1  # initial error threshold\n",
    "min_samples = 100 # min samples from the posterior\n",
    "steps = 60 # max number of iterations\n",
    "minAccRate = 0.01 # minimum acceptance rate to stop the iterations\n",
    "parallel = False # if parallel processing\n",
    "n_procs = 1 # number of processor for parallel processing (set to 1 if there is no parallel processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6be3a-5b26-4729-9a0c-5d3807f4f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model object\n",
    "class MyModel(abcTau.Model):\n",
    "\n",
    "    #This method initializes the model object.  \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # draw samples from the prior. \n",
    "    def draw_theta(self):\n",
    "        theta = []\n",
    "        for p in self.prior:\n",
    "            theta.append(p.rvs())\n",
    "        return theta\n",
    "\n",
    "    # Choose the generative model (from generative_models)\n",
    "    # Choose autocorrelation computation method (from basic_functions)\n",
    "    def generate_data(self, theta):\n",
    "        # generate synthetic data\n",
    "        if disp == None:\n",
    "            syn_data, numBinData =  eval('abcTau.generative_models.' + generativeModel + \\\n",
    "                                         '(theta, deltaT, binSize, T, numTrials, data_mean, data_var)')\n",
    "        else:\n",
    "            syn_data, numBinData =  eval('abcTau.generative_models.' + generativeModel + \\\n",
    "                                         '(theta, deltaT, binSize, T, numTrials, data_mean, data_var, disp)')\n",
    "               \n",
    "        # compute the summary statistics\n",
    "        syn_sumStat = abcTau.summary_stats.comp_sumStat(syn_data, summStat_metric, ifNorm, deltaT, binSize, T,\\\n",
    "                                          numBinData, maxTimeLag)   \n",
    "        return syn_sumStat\n",
    "\n",
    "    # Computes the summary statistics\n",
    "    def summary_stats(self, data):\n",
    "        sum_stat = data\n",
    "        return sum_stat\n",
    "\n",
    "    # Choose the method for computing distance (from basic_functions)\n",
    "    def distance_function(self, data, synth_data):\n",
    "        if np.nansum(synth_data) <= 0: # in case of all nans return large d to reject the sample\n",
    "            d = 10**4\n",
    "        else:\n",
    "            d = eval('abcTau.distance_functions.' +distFunc + '(data, synth_data)')        \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3fe493-0b34-4d52-bcf4-a6dd43dd88c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for loading and saving data\n",
    "datasave_path = 'Q:\\\\Personal\\\\Irina\\\\projects\\\\isttc\\\\results\\\\synthetic_data\\\\'\n",
    "inter_save_direc = datasave_path\n",
    "inter_filename = 'inter_filename'\n",
    "\n",
    "filename = 'test_1_trial'\n",
    "filenameSave = filename\n",
    "\n",
    "disp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a560189f-12fd-4adb-bece-214099946119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with aABC algorithm for any generative model\n",
    "abc_results, final_step = abcTau.fit.fit_withABC(MyModel, data_sumStat, priorDist, inter_save_direc, inter_filename,\\\n",
    "                                                 datasave_path,filenameSave, epsilon_0, min_samples, \\\n",
    "                                                 steps, minAccRate, parallel, n_procs, disp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc7b0b-3f71-442a-a0dc-e5acc5b97428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract estimated parameters\n",
    "theta_accepted = abc_results[final_step-1]['theta accepted']\n",
    "tau1 = theta_accepted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b5f49-2bd6-4b34-85c5-c28d96f3ec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ba017-342a-4ec9-8743-7dcc1e66f035",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tau1, density=True, label = r'Estimated')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
