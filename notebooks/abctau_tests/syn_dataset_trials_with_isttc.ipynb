{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using abcTau to fit ACFs for trials (Figure 2 from the paper). \n",
    "\n",
    "Three options to do that:\n",
    "* use abcTau package for both ACF and fitting\n",
    "* use ACF calculated before using acf function\n",
    "* use ACF calculated before using iSTTC concat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# add the path to the abcTau package\n",
    "import sys\n",
    "#sys.path.append('./abcTau')\n",
    "sys.path.append('C:\\\\Users\\\\ipochino\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\isttc\\\\Lib\\\\site-packages\\\\abcTau') # IP: replaced previous line with that; relative path was not working\n",
    "import abcTau\n",
    "\n",
    "from isttc.scripts.cfg_global import project_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = project_folder_path + 'synthetic_dataset\\\\'\n",
    "results_folder = project_folder_path + 'results\\\\synthetic\\\\results\\\\param_fr_alpha_tau\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data (spike trains and calculated acf's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_folder + 'trials40.pkl','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "trial_dict = data['trial_dict']\n",
    "alphas = data['alphas']\n",
    "fr_values = data['fr_values']\n",
    "taus_ms = data['tau_ms']\n",
    "n_trials = data['n_trials']\n",
    "trial_lens = data['trial_lens']\n",
    "\n",
    "print(f'n spike trains {len(trial_dict)}, trial_lens {trial_lens[0]} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n spike trains 100000, trial_lens 1000 ms\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_folder + 'trials40_binned.pkl','rb') as f:\n",
    "    data_binned = pickle.load(f)\n",
    "\n",
    "trial_dict_binned = data_binned['trial_dict']\n",
    "alphas_binned  = data_binned['alphas']\n",
    "fr_values_binned  = data_binned['fr_values']\n",
    "taus_ms_binned  = data_binned['tau_ms']\n",
    "n_trials_binned  = data_binned['n_trials']\n",
    "trial_lens_binned  = data_binned['trial_lens']\n",
    "\n",
    "print(f'n spike trains {len(trial_dict_binned)}, trial_lens {trial_lens_binned[0]} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_folder + 'tau_pearsonr_trial_50ms_20lags_dict.pkl', \"rb\") as f:\n",
    "    pearsonr_trial_avg_dict = pickle.load(f)\n",
    "\n",
    "with open(results_folder + 'tau_isttc_trial_concat_50ms_20lags_dict.pkl', \"rb\") as f:\n",
    "    sttc_trial_concat_dict = pickle.load(f)\n",
    "\n",
    "print(f'len pearsonr_trial_avg_dict {len(pearsonr_trial_avg_dict)}')\n",
    "print(f'len sttc_trial_concat_dict {len(sttc_trial_concat_dict)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### abcTau for both ACF and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parameters\n",
    "summStat_metric = 'comp_cc'\n",
    "ifNorm = True # if normalize the autocorrelation or PSD\n",
    "deltaT = 1 # temporal resolution of data.\n",
    "binSize = 50 #  bin-size for binning data and computing the autocorrelation.\n",
    "disp = None # put the dispersion parameter if computed with grid-search\n",
    "maxTimeLag = 1000 # only used when using autocorrelation for summary statistics\n",
    "#lm = round(maxTimeLag/binSize) # maximum bin for autocorrelation computation\n",
    "\n",
    "# desired generative model from the list of 'generative_models.py' and the distance function from 'diatance_functions.py'\n",
    "generativeModel = 'oneTauOU_gammaSpikes'\n",
    "distFunc = 'linear_distance'\n",
    "\n",
    "# Define a uniform prior distribution over the given range\n",
    "# for a uniform prior: stats.uniform(loc=x_min,scale=x_max-x_min)\n",
    "t_min = 0.0 # first timescale\n",
    "t_max = 400.0\n",
    "priorDist = [stats.uniform(loc= t_min, scale = t_max - t_min)]\n",
    "\n",
    "# aABC fitting parameters\n",
    "epsilon_0 = 1  # initial error threshold\n",
    "min_samples = 100 # min samples from the posterior\n",
    "steps = 60 # max number of iterations\n",
    "minAccRate = 0.01 # minimum acceptance rate to stop the iterations\n",
    "parallel = False # if parallel processing\n",
    "n_procs = 1 # number of processor for parallel processing (set to 1 if there is no parallel processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Create the model object: Just copy paste the following (this a general definition of the model object, but all parts \n",
    "# including generative models, summary statistics computation or distance function can be replaced by your own functions. \n",
    "# You can add your handmade functions inside respected modules: \"generative_models.py\", \"distance_functions.py\", \"summary_stats.py\")\n",
    "\n",
    "# creating model object\n",
    "class MyModel(abcTau.Model):\n",
    "\n",
    "    #This method initializes the model object.  \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # draw samples from the prior. \n",
    "    def draw_theta(self):\n",
    "        theta = []\n",
    "        for p in self.prior:\n",
    "            theta.append(p.rvs())\n",
    "        return theta\n",
    "\n",
    "    # Choose the generative model (from generative_models)\n",
    "    # Choose autocorrelation computation method (from basic_functions)\n",
    "    def generate_data(self, theta):\n",
    "        # generate synthetic data\n",
    "        if disp == None:\n",
    "            syn_data, numBinData =  eval('abcTau.generative_models.' + generativeModel + \\\n",
    "                                         '(theta, deltaT, binSize, T, numTrials, data_mean, data_var)')\n",
    "        else:\n",
    "            syn_data, numBinData =  eval('abcTau.generative_models.' + generativeModel + \\\n",
    "                                         '(theta, deltaT, binSize, T, numTrials, data_mean, data_var, disp)')\n",
    "               \n",
    "        # compute the summary statistics\n",
    "        syn_sumStat = abcTau.summary_stats.comp_sumStat(syn_data, summStat_metric, ifNorm, deltaT, binSize, T,\\\n",
    "                                          numBinData, maxTimeLag)   \n",
    "        return syn_sumStat\n",
    "\n",
    "    # Computes the summary statistics\n",
    "    def summary_stats(self, data):\n",
    "        sum_stat = data\n",
    "        return sum_stat\n",
    "\n",
    "    # Choose the method for computing distance (from basic_functions)\n",
    "    def distance_function(self, data, synth_data):\n",
    "        if np.nansum(synth_data) <= 0: # in case of all nans return large d to reject the sample\n",
    "            d = 10**4\n",
    "        else:\n",
    "            d = eval('abcTau.distance_functions.' +distFunc + '(data, synth_data)')        \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = trial_dict_binned[0] \n",
    "k = 0\n",
    "\n",
    "spike_binned = v[0]\n",
    "numTrials = n_trials_binned[k]\n",
    "T = trial_lens_binned[k]\n",
    "\n",
    "numBinData = spike_binned.shape[1]\n",
    "data_mean = np.mean(spike_binned)\n",
    "data_var = abcTau.preprocessing.comp_cc(spike_binned, spike_binned, 1, binSize, numBinData)[0]\n",
    "data_sumStat = abcTau.summary_stats.comp_sumStat(spike_binned, summStat_metric, ifNorm, deltaT, binSize, T, numBinData, maxTimeLag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.60755511,  0.28021065,  0.06623287, -0.00929441,\n",
       "       -0.06664008, -0.1502196 , -0.14810597, -0.24018935, -0.1930169 ,\n",
       "       -0.07217865,  0.01001045,  0.01973116,  0.02658514,  0.03027752,\n",
       "        0.05051176,  0.03904692,  0.02510819, -0.00332314,  0.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sumStat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.878139534883721\n"
     ]
    }
   ],
   "source": [
    "disp = np.var(spike_binned)/ data_mean\n",
    "print(disp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'theta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m syn_data, numBinData \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28meval\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabcTau.generative_models.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m generativeModel \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m      2\u001b[0m                                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(theta, deltaT, binSize, T, numTrials, data_mean, data_var, disp)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m<string>:1\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'theta' is not defined"
     ]
    }
   ],
   "source": [
    "syn_data, numBinData =  eval('abcTau.generative_models.' + generativeModel + \\\n",
    "                                         '(theta, deltaT, binSize, T, numTrials, data_mean, data_var, disp)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder_abctau = project_folder_path + 'results\\\\synthetic\\\\results\\\\param_fr_alpha_tau_abctau\\\\'\n",
    "# path and filename to save the intermediate results after running each step\n",
    "inter_save_direc = results_folder_abctau + 'interim_results\\\\'\n",
    "datasave_path = results_folder_abctau + 'final_results\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in list(trial_dict_binned.items())[0:100]:\n",
    "    spike_binned = v[0]\n",
    "    numTrials = n_trials_binned[k]\n",
    "    T = trial_lens_binned[k]\n",
    "\n",
    "    numBinData = spike_binned.shape[1]\n",
    "    data_mean = np.mean(spike_binned)\n",
    "    data_var = abcTau.preprocessing.comp_cc(spike_binned, spike_binned, 1, binSize, numBinData)[0]\n",
    "    data_sumStat = abcTau.summary_stats.comp_sumStat(spike_binned, summStat_metric, ifNorm, deltaT, binSize, T, numBinData, maxTimeLag)\n",
    "    \n",
    "    # data_sumStat, data_mean, data_var, T, numTrials = abcTau.preprocessing.extract_stats(binary_train, deltaT, binSize,\\\n",
    "    #                                                                                   summStat_metric, ifNorm, maxTimeLag)\n",
    "    print(f'sumStat len {data_sumStat.shape}, data_mean {data_mean}, data_var {data_var}, T {T}, numTrials {numTrials}, numBinData {numBinData}')\n",
    "\n",
    "    # Run the aABC algorithm and save the results\n",
    "    filenameSave = 'spike_train_' + str(k)\n",
    "    inter_filename = 'spike_train_interim_' + str(k)\n",
    "    abc_results, final_step = abcTau.fit.fit_withABC(MyModel, data_sumStat, priorDist, inter_save_direc, inter_filename,\\\n",
    "                                                     datasave_path, filenameSave, epsilon_0, min_samples, \\\n",
    "                                                     steps, minAccRate, parallel, n_procs, disp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results\n",
    "\n",
    "(took examples from other tutorials - I downloaded them before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load abc results\n",
    "filename = 'test_steps24'\n",
    "abc_results = np.load(datasave_path + filename + '.npy', allow_pickle=True)\n",
    "ind = filename.find('steps') \n",
    "final_step = int(filename[ind+5] + filename[ind+6])\n",
    "\n",
    "# extract estimated parameters\n",
    "theta_accepted = abc_results[final_step-1]['theta accepted']\n",
    "tau1 = theta_accepted[0]\n",
    "\n",
    "# extract the development of the error threshold and acceptance rate during fitting (discarding first iteration)\n",
    "eps = []\n",
    "accR = []\n",
    "steps = np.arange(1,final_step+1)[1:]\n",
    "for i in range(final_step): \n",
    "    step_results = abc_results[i]\n",
    "    eps.append(step_results['epsilon'])\n",
    "    accR.append(step_results['n accepted']/step_results['n total'])\n",
    "eps = eps[1:]\n",
    "accR = accR[1:]\n",
    "\n",
    "# ground truth values \n",
    "tau1_gt = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tau1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize = (24,6))\n",
    "\n",
    "# plotting variables\n",
    "cABC = 'darkorange'\n",
    "ctheor = 'gray'\n",
    "cAccR = 'r'\n",
    "cErr = 'b'\n",
    "a = 0.5\n",
    "lw = 3\n",
    "\n",
    "ax = plt.subplot(141)\n",
    "ax.hist(tau1, facecolor= cABC, density=True, alpha = a, label = r'Estimated')\n",
    "sns.kdeplot(tau1, color = cABC)\n",
    "plt.axvline(tau1_gt, color = ctheor, label = r'Ground truth', linewidth = lw, linestyle = '--')\n",
    "\n",
    "ax.set_xlabel(r'$\\tau_1$ [ms]')\n",
    "ax.set_ylabel('Probability density')\n",
    "# Hide the right and top spines\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.legend(frameon = False, bbox_to_anchor=(0.4, 1))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sttc_results_folder = 'E:\\\\work\\\\q_backup_06_03_2025\\\\projects\\\\isttc\\\\results\\\\synthetic\\\\results\\\\param_fr_alpha_tau\\\\'\n",
    "\n",
    "df_all = pd.read_pickle(sttc_results_folder + \"summary_tau_all_long_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all['unit_id'] == 10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
