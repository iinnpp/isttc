{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "821b2ec7-71a8-4c9e-ac53-d569c966e511",
   "metadata": {},
   "source": [
    "We do not have ground truth here but we have the high quality units from neuropixels. The idea is to show that the STTC concat \n",
    "works better compared to the trial average methods (both Pearson and STTC).\n",
    "\n",
    "Steps:\n",
    "0. Calculate STTC and ACF on the full signal ()\n",
    "1. Take the full signal and randomly pick N trials\n",
    "2. Calculate Pearson trial-average\n",
    "3. Calculate STTC trial-average\n",
    "4. Calculate STTC on concatenated data\n",
    "5. Compare 2,3,4, values calculated on full signa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39be3909-302c-4ae0-ac5a-b8e950540326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from scipy.optimize import curve_fit, OptimizeWarning\n",
    "\n",
    "import warnings\n",
    "\n",
    "# import from scripts\n",
    "import os\n",
    "os.chdir(os.path.expanduser(\"D:\\\\intr_timescales\\\\isttc\\\\scripts\"))\n",
    "#os.chdir(os.path.expanduser(\"C:\\\\Users\\\\ipoch\\\\Documents\\\\repos\\\\isttc\\\\scripts\"))\n",
    "from calculate_acf import acf_sttc, acf_pearsonr_trial_avg, acf_sttc_trial_avg, acf_sttc_trial_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3fc2dd-d44e-4264-95e8-5781648f0228",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b9b6f5-9174-4dc3-ab9b-6ca0e4d1d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'Q:\\\\Personal\\\\Irina\\\\projects\\\\isttc\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1acf8d7-f703-4761-91e1-f94c5ab33285",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data_file = data_folder + 'allen_test_full_v3.csv'\n",
    "with open(csv_data_file, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    sua_list = list(reader)\n",
    "print(f'Loaded N units {len(sua_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e1715-f9fc-4c03-9a4d-e34e5edd7825",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 30000 # raw neuropixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8aaaeb-5c11-46de-9814-c9ad31933845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are spikes after 30 mins of recording\n",
    "\n",
    "for i in range(len(sua_list)):\n",
    "    spike_train_ = np.asarray(sua_list[i][4:]).astype(float)\n",
    "    spike_train_fs = spike_train_ * fs  # csv is in sec\n",
    "    spike_train_fs_int = list(spike_train_fs.astype(int))\n",
    "    if spike_train_fs_int[-1] >= 30 * 60 * fs:\n",
    "        print(f'spike >= 30 min {spike_train_fs_int[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce5fcf7-dbe0-47fc-abf6-d3a2324273df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use first 30 min of the signal\n",
    "sua_list_30min = []\n",
    "\n",
    "for i in range(len(sua_list)):\n",
    "    spike_train_ = np.asarray(sua_list[i][4:]).astype(float)\n",
    "    spike_train_fs = spike_train_ * fs  # csv is in sec\n",
    "    spike_train_fs_int = spike_train_fs.astype(int)\n",
    "    # if spike_train_fs_int[-1] >= 30 * 60 * fs:\n",
    "    #     print(f'spike >= 30 min {spike_train_fs_int[-1]}')\n",
    "    sua_list_30min.append(spike_train_fs_int[spike_train_fs_int < 30 * 60 * fs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40db0c3-f2a8-409a-9fd7-bdeb40b8d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin the data\n",
    "bin_size = 50 # in ms\n",
    "fs = 30000 # raw neuropixels\n",
    "signal_len = 30 * 60 * fs\n",
    "\n",
    "verbose = False\n",
    "\n",
    "sua_list_30min_binned_l = []\n",
    "\n",
    "for j in range(len(sua_list_30min)):\n",
    "    bin_length_fs = int(fs / 1000 * bin_size)\n",
    "    n_bin_edges =  int(signal_len/bin_length_fs)\n",
    "    bins = np.linspace(0, bin_length_fs * n_bin_edges, n_bin_edges + 1).astype(int)\n",
    "    binned_spike_train, _ = np.histogram(sua_list_30min[j], bins)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Binning spike train: bin_length_ms {}, bin_length_fs {}'.format(bin_size, bin_length_fs))\n",
    "        print('n bins {}, spike bin count: number of spikes in bin - number of bins {}'.format(binned_spike_train.shape,\n",
    "                                                                                               np.unique(\n",
    "                                                                                                   binned_spike_train,\n",
    "                                                                                                   return_counts=True)))\n",
    "    sua_list_30min_binned_l.append(binned_spike_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7703d08-7b88-43b5-a3bb-bcd7d4b2127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "np.save(data_folder + 'results\\\\allen_mice\\\\dataset_full_split_check_30min\\\\sua_list_30min.npy', \n",
    "        np.asarray(sua_list_30min, dtype='object'))\n",
    "np.save(data_folder + 'results\\\\allen_mice\\\\dataset_full_split_check_30min\\\\sua_list_30min_binned.npy', \n",
    "        sua_list_30min_binned_l, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f8c826-a2bf-4a8f-a5ed-9204f150d982",
   "metadata": {},
   "source": [
    "### Reload binned and non binned data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47aebc68-0b85-4709-b9f1-78ba7e3d7100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_single_exp(ydata_to_fit_, start_idx_=1):\n",
    "    \"\"\"\n",
    "    Fit function func_exp to data using non-linear least square.\n",
    "\n",
    "    todo check that - important point: Fit is done from the first ACF value (acf[0] is skipped, it is done like this\n",
    "    in the papers, still not sure)\n",
    "\n",
    "    :param ydata_to_fit_: 1d array, the dependant data to fit\n",
    "    :param start_idx_: int, index to start fitting from\n",
    "    :return: fit_popt, fit_pcov, tau, fit_r_squared\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, len(ydata_to_fit_), len(ydata_to_fit_)).astype(int)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('error')\n",
    "        try:\n",
    "            popt, pcov = curve_fit(func_single_exp, t[start_idx_:], ydata_to_fit_[start_idx_:], maxfev=5000)\n",
    "            fit_popt = popt\n",
    "            fit_pcov = pcov\n",
    "            tau = 1 / fit_popt[1]\n",
    "        except RuntimeError as e:\n",
    "            print('RuntimeError: {}'. format(e))\n",
    "            fit_popt, fit_pcov, tau, fit_r_squared = np.nan, np.nan, np.nan, np.nan\n",
    "        except OptimizeWarning as o:\n",
    "            print('OptimizeWarning: {}'. format(o))\n",
    "            fit_popt, fit_pcov, tau, fit_r_squared = np.nan, np.nan, np.nan, np.nan\n",
    "        except RuntimeWarning as re:\n",
    "            print('RuntimeWarning: {}'. format(re))\n",
    "            fit_popt, fit_pcov, tau, fit_r_squared = np.nan, np.nan, np.nan, np.nan\n",
    "        except ValueError as ve:\n",
    "            print('ValueError: {}'. format(ve))\n",
    "            print('Possible reason: acf contains NaNs, low spike count')\n",
    "            fit_popt, fit_pcov, tau, fit_r_squared = np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    return tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb0da217-e436-4c9c-97a3-30d189a3f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sua_list_30min = np.load(data_folder + 'results\\\\allen_mice\\\\dataset_full_split_check_30min\\\\sua_list_30min.npy', allow_pickle=True)\n",
    "sua_list_30min_binned = np.load(data_folder + 'results\\\\allen_mice\\\\dataset_full_split_check_30min\\\\sua_list_30min_binned.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd8dc5b6-15f9-41f2-92db-157948921024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len sua 18168, len sua_binned 18168\n"
     ]
    }
   ],
   "source": [
    "print(f'len sua {len(sua_list_30min)}, len sua_binned {len(sua_list_30min_binned)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcf95d9c-1608-4c8e-bd63-d443bbe165f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lags = 20\n",
    "fs = 30000 # raw neuropixels\n",
    "bin_size = 50 * (fs / 1000)\n",
    "sttc_dt = 49 * (fs / 1000)\n",
    "signal_len = 30 * 60 * fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ae45722-129a-452a-93d8-e0a1a86ef7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_exp_abc_like(x, a, tau):\n",
    "    return a * np.exp(-x/tau) \n",
    "\n",
    "def func_single_exp_monkey_like(x, a, b, c):\n",
    "    #return a * np.exp(-b * x) + c\n",
    "    return a * (np.exp(-b * x) + c) # as in the paper\n",
    "\n",
    "def func_single_exp(x, a, b, c):\n",
    "    return a * np.exp(-b * x) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7634f92-d1f3-4d85-90b9-36df4f111c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n"
     ]
    }
   ],
   "source": [
    "t_axes = np.linspace(0,num_lags,num_lags+1).astype(int)\n",
    "print(t_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3dcbf8-c87b-4b8e-b6f0-945f56c40a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n"
     ]
    }
   ],
   "source": [
    "sttc_full_tau_ms_l = []\n",
    "acf_full_tau_ms_l = []\n",
    "\n",
    "for k in range(len(sua_list_30min_binned)):\n",
    "    # Using acf func\n",
    "    spike_train_binned_acf = acf(sua_list_30min_binned[k], nlags=num_lags)\n",
    "    #print('spike_train_binned_acf shape {}, \\nspike_train_binned_acf: {}'.format(spike_train_binned_acf.shape, spike_train_binned_acf))\n",
    "    \n",
    "    # Using isttc\n",
    "    spike_train_acf = acf_sttc(sua_list_30min[k], num_lags, lag_shift_=bin_size, sttc_dt_=sttc_dt, signal_length_=signal_len, verbose_=False)\n",
    "    #print('spike_train_acf shape {}, \\nspike_train_acf: {}'.format(len(spike_train_acf), spike_train_acf))\n",
    "    \n",
    "    # calculate tau\n",
    "    spike_train_binned_tau = fit_single_exp(spike_train_binned_acf, start_idx_=1)\n",
    "    spike_train_binned_tau_ms = spike_train_binned_tau * bin_size\n",
    "    #print('spike_train_binned_tau_ms: {}'.format(spike_train_binned_tau_ms))\n",
    "    \n",
    "    spike_train_popt_tau = fit_single_exp(spike_train_acf, start_idx_=1)\n",
    "    spike_train_tau_ms = spike_train_popt_tau * bin_size\n",
    "    #print('spike_train_tau_ms: {}'.format(spike_train_tau_ms))\n",
    "\n",
    "    sttc_full_tau_ms_l.append(spike_train_tau_ms)\n",
    "    acf_full_tau_ms_l.append(spike_train_binned_tau_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898f941-f035-4ad7-b638-37694992b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save lists with taus\n",
    "np.save(data_folder + 'results\\\\allen_mice\\\\dataset_full_split_check_30min\\\\sttc_full_tau_ms_l.npy', \n",
    "        sttc_full_tau_ms_l, , allow_pickle=True)\n",
    "np.save(data_folder + 'results\\\\allen_mice\\\\dataset_full_split_check_30min\\\\acf_full_tau_ms_l.npy', \n",
    "        acf_full_tau_ms_l, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4e39d1-e164-4326-b1b5-55536ffb006f",
   "metadata": {},
   "source": [
    "### Make trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a46a6-ccfb-424d-8411-da21ce86792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trials(spike_times_, signal_len_, n_trials_, trial_len_, verbose_=False):\n",
    "    # get random trail starts and ends\n",
    "    trials_start = [randrange(0, signal_len_-trial_len_+1) for i in range(n_trials_)]\n",
    "    trials_end = [trial_start + trial_len_ for trial_start in trials_start]\n",
    "    trial_intervals = np.vstack((trials_start, trials_end)).T\n",
    "    if verbose_:\n",
    "        print('N trials {}, trail len {}, n trial starts {}, \\ntrial starts {}, \\ntrial starts {}'.format(n_trials_, trial_len_, \n",
    "                                                                                                          len(trials_start), \n",
    "                                                                                                          trials_start, trials_end))\n",
    "    # get spikes\n",
    "    spikes_trials = []\n",
    "    for i in range(n_trials_):\n",
    "        spikes_trial = spike_times_[np.logical_and(spike_times_ >= trial_intervals[i,0], spike_times_ < trial_intervals[i,1])]\n",
    "        spikes_trials.append(spikes_trial)\n",
    "\n",
    "    # realign all trails to start with 0\n",
    "    spikes_trials_realigned_l = []\n",
    "    for idx, trial in enumerate(spikes_trials):\n",
    "        spikes_trial_realigned = trial - trial_intervals[idx,0] \n",
    "        spikes_trials_realigned_l.append(spikes_trial_realigned)\n",
    "\n",
    "    return spikes_trials_realigned_l\n",
    "\n",
    "def bin_trials(spikes_trials_l_, trial_len_, bin_size_):\n",
    "    binned_spikes_trials_l = []\n",
    "\n",
    "    n_bin_edges =  int(trial_len_/bin_size_)\n",
    "    bins_ = np.linspace(0, bin_size * n_bin_edges, n_bin_edges + 1).astype(int)\n",
    "    for trial in spikes_trials_l_:\n",
    "        binned_spike_train, _ = np.histogram(trial, bins_)\n",
    "        binned_spikes_trials_l.append(binned_spike_train)\n",
    "    binned_spikes_trials_2d = np.asarray(binned_spikes_trials_l)\n",
    "\n",
    "    return binned_spikes_trials_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0973fb6-c9db-40b2-8420-90529b46d0e1",
   "metadata": {},
   "source": [
    "### Run for one trial realization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
