{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565655e6-ef4f-4e85-9d31-d6373faf37ef",
   "metadata": {},
   "source": [
    "Estimate parameters for resampling procedure (trial generation).\n",
    "\n",
    "* Number of resampling iterations: M is based on bootstrapping stability analysis \n",
    "* Number of trials per resampling: N = 40 (based on data in monkey dataset so the number of trials is from experiments)\n",
    "\n",
    "#### Bootstrapping Stability Analysis (for M)\n",
    "\n",
    "We determine a reasonable M by checking when the median stabilizes (using median because IT distribution is skewed):\n",
    "\n",
    "1. Compute the median and the sem equivalent for median (using MAD - median absolute deviation) of intrinsic timescales as M increases.\n",
    "2. Plot the standard error of the median vs. M.\n",
    "3. Pick M where SE and its 95% confidence interval are under 25ms.\n",
    "\n",
    "Run for M = [50, 100, 200, 500, 1000]\n",
    "\n",
    "#### Selecting number of signals for M estimate\n",
    "\n",
    "Spike trains are coming from different brain areas and may have different variance so using only one is not gut. The total number of signals and the min per area number of signals are set. After sampling the min number per area the rest is sampled proportionally to the number of signals in the area.\n",
    "\n",
    "Min number per area: 10\n",
    "N total: 100 (97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52fab37d-1d77-44d6-9bbc-73c2dd812443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import from scripts\n",
    "import os\n",
    "current_wd = os.getcwd()\n",
    "os.chdir(os.path.abspath(\"..\\\\..\\\\..\\\\isttc\\\\scripts\"))\n",
    "#os.chdir(os.path.abspath(\"C:\\\\Users\\\\ipoch\\\\Documents\\\\repos\\\\isttc\\\\scripts\"))\n",
    "from calculate_tau import fit_single_exp, func_single_exp_monkey\n",
    "from cfg_global import project_folder_path\n",
    "from calculate_acf import acf_pearsonr_trial_avg, acf_sttc_trial_avg, acf_sttc_trial_concat\n",
    "from spike_train_utils import bin_spike_train_fixed_len, get_trials, bin_trials\n",
    "os.chdir(current_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eced2eb-b28e-4606-9f21-a0f0a186b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = project_folder_path + 'results\\\\allen_mice\\\\dataset\\\\cut_30min\\\\'\n",
    "fig_folder = project_folder_path + 'results\\\\allen_mice\\\\fig_draft_paper\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4044772d-6687-43a9-aea5-9e391eebeaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_m = False\n",
    "save_fig = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd5a8a0-df42-4ee8-8a2e-a7603bc6d72d",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3adff86-673a-4630-a044-eafd94f0775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if estimate_m:\n",
    "    csv_data_file = dataset_folder + 'sua_list_constrained.csv'\n",
    "    with open(csv_data_file, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        sua_list = list(reader)\n",
    "    print(f'Loaded N units {len(sua_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8151c56-7fd2-4339-93cb-3af2c7486849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>specimen_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>unit_id</th>\n",
       "      <th>ecephys_structure_acronym</th>\n",
       "      <th>firing_rate</th>\n",
       "      <th>amplitude_cutoff</th>\n",
       "      <th>isi_violations</th>\n",
       "      <th>presence_ratio</th>\n",
       "      <th>fr_hz_spont_30min</th>\n",
       "      <th>fr_hz_spont_30min_log10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>744912849</td>\n",
       "      <td>766640955</td>\n",
       "      <td>950913540</td>\n",
       "      <td>VISam</td>\n",
       "      <td>5.924830</td>\n",
       "      <td>0.004951</td>\n",
       "      <td>0.157933</td>\n",
       "      <td>0.99</td>\n",
       "      <td>6.038333</td>\n",
       "      <td>0.780917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>744912849</td>\n",
       "      <td>766640955</td>\n",
       "      <td>950915005</td>\n",
       "      <td>VISam</td>\n",
       "      <td>4.549072</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.096072</td>\n",
       "      <td>0.99</td>\n",
       "      <td>4.030556</td>\n",
       "      <td>0.605365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   specimen_id session_id    unit_id ecephys_structure_acronym  firing_rate  \\\n",
       "66   744912849  766640955  950913540                     VISam     5.924830   \n",
       "67   744912849  766640955  950915005                     VISam     4.549072   \n",
       "\n",
       "    amplitude_cutoff  isi_violations  presence_ratio  fr_hz_spont_30min  \\\n",
       "66          0.004951        0.157933            0.99           6.038333   \n",
       "67          0.001203        0.096072            0.99           4.030556   \n",
       "\n",
       "    fr_hz_spont_30min_log10  \n",
       "66                 0.780917  \n",
       "67                 0.605365  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units_info_df = pd.read_pickle(dataset_folder + 'sua_list_constrained_units_df.pkl')\n",
    "units_info_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5fac7d-857f-49bf-a9bc-07e765f6cbc0",
   "metadata": {},
   "source": [
    "### Estimate M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1340d06-c005-49f5-930d-be7b11cf34f1",
   "metadata": {},
   "source": [
    "#### Run bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8fd2bf-d7f5-4539-a6d0-6882a44f66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_signals(units_df, min_per_area, n_total):\n",
    "    sampled_units = []\n",
    "\n",
    "    area_counts = units_df[\"ecephys_structure_acronym\"].value_counts()\n",
    "    base_samples = {area: min(min_per_area, count) for area, count in area_counts.items()}\n",
    "    remaining_samples = n_total - sum(base_samples.values())\n",
    "    total_remaining = sum(area_counts) - sum(base_samples.values())\n",
    "    print(f'total {n_total}, min per area {min_per_area}. Base samples {base_samples}, \\nproportional samples {remaining_samples}')\n",
    "    \n",
    "    # get min_per_area, then sample proportionally\n",
    "    for area, count in area_counts.items():\n",
    "        base_samples = min(min_per_area, count)\n",
    "        extra_samples = int((count / total_remaining) * remaining_samples) if total_remaining > 0 else 0\n",
    "        n_samples = base_samples + extra_samples\n",
    "        sampled_units.extend(units_info_df[units_info_df[\"ecephys_structure_acronym\"] == area]\n",
    "                             .sample(n=min(n_samples, count), random_state=42)[\"unit_id\"].tolist())\n",
    "    \n",
    "    return sampled_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714adf76-4b1e-47e7-ae68-567db2ecba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if estimate_m:\n",
    "    fs = 30000\n",
    "    signal_len = int(30 * 60 * fs)\n",
    "    n_lags = 20\n",
    "    bin_size = 50 # in ms\n",
    "    trial_len = int(n_lags * bin_size * (fs/1000))\n",
    "    \n",
    "    n_trials = 40 # this is fixed based on experimental datasets\n",
    "    m_iterations = [20, 40, 60, 80, 100, 150, 200, 500, 1000]\n",
    "    \n",
    "    n_total_signals = 100\n",
    "    min_signal_per_area = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c8f3a-58d4-4edf-b11f-cccf8a23d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "if estimate_m:\n",
    "    units_to_sample = sample_signals(units_info_df, min_signal_per_area, n_total_signals)\n",
    "    random_signals = [item for item in sua_list if item[2] in units_to_sample]\n",
    "    \n",
    "    output_log = dataset_folder + f'resampling//resampling_params_estimate_{n_signals}_signals.txt'\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = open(output_log, 'w')\n",
    "    \n",
    "    signal_tau_dict = {}\n",
    "    \n",
    "    for signal_idx, signal in enumerate(random_signals):\n",
    "        print(f'###\\nCalculating for {signal_idx} signal')\n",
    "        spikes = np.asarray([int(spike) for spike in signal[8:]])\n",
    "    \n",
    "        tau_dict = {}\n",
    "        for m_iteration in m_iterations:\n",
    "            print(f'calculating for {m_iteration} resampling iterations')\n",
    "            tau_l = []\n",
    "            for m in range(m_iteration):\n",
    "                spikes_trials = get_trials(spikes, signal_len, n_trials, trial_len, verbose_=False)\n",
    "                spikes_trials_binned = bin_trials(spikes_trials, trial_len, int(bin_size*(fs/1000)))\n",
    "                # get taus\n",
    "                _, acf_average = acf_pearsonr_trial_avg(spikes_trials_binned, n_lags, verbose_=False)\n",
    "                _, _, tau, _, _, _, _ = fit_single_exp(acf_average, start_idx_=1, exp_fun_=func_single_exp_monkey)\n",
    "                tau_l.append(tau)\n",
    "            tau_dict[m_iteration] = tau_l\n",
    "            \n",
    "        signal_tau_dict[signal[2]] = tau_dict\n",
    "    \n",
    "    with open(dataset_folder + f'resampling//signal_tau_dict_{n_signals}_signals.pkl', \"wb\") as f:\n",
    "        pickle.dump(signal_tau_dict, f)\n",
    "    \n",
    "    sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53f7bd-b9b6-497a-82c4-62ac29e896eb",
   "metadata": {},
   "source": [
    "#### Find M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f772fded-c936-4aa8-92b4-02e3b904766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N signals 97\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_folder + f'resampling//signal_tau_dict_100_signals_pearsonr.pkl', \"rb\") as f:\n",
    "    signal_tau_dict = pickle.load(f)\n",
    "signal_tau_dict.keys()\n",
    "print(f'N signals {len(signal_tau_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a25a7010-9e69-42ee-ae37-51f11aebf505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['950914624', '950923381', '950924658', '950938932', '950939017', '950939830', '950944338', '950917640', '950917714', '950928487', '950924978', '950935732', '950937226', '950942561', '950944107', '950954735', '950925317', '950935960', '950989951', '950990251', '951000632', '951003839', '950991136', '950999282', '950999410', '951021971', '951024089', '951002677', '951035794', '951037928', '951046945', '950999050', '951006498', '951005379', '951013444', '951024251', '950997486', '950997679', '951009926', '951010065', '951015246', '951020151', '951020751', '951004012', '951005516', '951006603', '951007918', '951014958', '951035727', '951062499', '951065724', '951070584', '951070648', '951083960', '951085015', '951085222', '951088359', '951104974', '951141797', '951147592', '951155468', '951139932', '951142334', '951142465', '951144025', '951144129', '951144152', '951146027', '951137420', '951139179', '951142875', '951142908', '951143408', '951143427', '951084273', '951085495', '951131534', '951132419', '951133746', '951136509', '951141109', '951086045', '951088913', '951101524', '951101562', '951166058', '951173250', '951175942', '951137609', '951140917', '951140992', '951153488', '951186434', '951186463', '951187147', '951187276', '951188341']\n"
     ]
    }
   ],
   "source": [
    "signals = list(signal_tau_dict.keys())\n",
    "print(signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6f12b-9927-4bf2-a9ec-0b3ee7a6440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_l = []\n",
    "m_iterations_l = []\n",
    "# tau_nanmean_l = []\n",
    "# sem_tau_l = []\n",
    "tau_nanmedian_l = []\n",
    "sem_tau_median_l = []\n",
    "\n",
    "for k, v in signal_tau_dict.items():\n",
    "    for kk, vv in v.items():\n",
    "        vv_nan_removed = [x for x in vv if not np.isnan(x)]\n",
    "        if len(vv_nan_removed) < 1:\n",
    "            print(len(vv_nan_removed))\n",
    "            tau_med = np.nan\n",
    "            tau_sem_median = np.nan\n",
    "        # # mean based \n",
    "        # tau_mean = np.mean(vv_nan_removed)\n",
    "        # tau_sem = stats.sem(vv_nan_removed)  \n",
    "        # median based\n",
    "        else:\n",
    "            tau_med = np.nanmedian(vv_nan_removed)\n",
    "            tau_mad = np.nanmedian(np.abs(vv_nan_removed - tau_med)) # median absolute deviation\n",
    "            tau_sem_median = (1.4826 * tau_mad) / np.sqrt(len(vv_nan_removed)) # sem of sorts\n",
    "        \n",
    "        signal_l.append(k)\n",
    "        m_iterations_l.append(kk)\n",
    "        # tau_nanmean_l.append(tau_mean)\n",
    "        # sem_tau_l.append(tau_sem)\n",
    "        tau_nanmedian_l.append(tau_med)\n",
    "        sem_tau_median_l.append(tau_sem_median)\n",
    "\n",
    "# Create tau_df\n",
    "tau_df = pd.DataFrame({\n",
    "    \"signal\": signal_l,\n",
    "    \"m_iterations\": m_iterations_l,\n",
    "    # \"tau_nanmean\": tau_nanmean_l,\n",
    "    # 'tau_sem': sem_tau_l,\n",
    "    \"tau_nanmedian\": tau_nanmedian_l,\n",
    "    'tau_sem_median': sem_tau_median_l\n",
    "})\n",
    "# tau_df['tau_sem_ms'] = tau_df['tau_sem'] * 50\n",
    "tau_df['tau_sem_median_ms'] = tau_df['tau_sem_median'] * 50\n",
    "\n",
    "# Check the stabilization of SE\n",
    "threshold = 25 # ms\n",
    "stabilization_list = []\n",
    "\n",
    "for signal in tau_df[\"signal\"].unique():\n",
    "    signal_data = tau_df.query('signal == @signal').sort_values(by='m_iterations')\n",
    "    sem_tau_values = signal_data['tau_sem_median_ms'].values\n",
    "    m_iterations_values = signal_data['m_iterations'].values\n",
    "    \n",
    "    stabilization_point = np.nan  # Default to NaN if no stabilization is found\n",
    "    for i in range(1, len(sem_tau_values)):\n",
    "        if sem_tau_values[i] <= threshold:\n",
    "            stabilization_point = m_iterations_values[i]\n",
    "            break\n",
    "    stabilization_list.append({\"signal\": signal, \"stabilization_point\": stabilization_point})\n",
    "\n",
    "# Create stabilization_df\n",
    "stabilization_df = pd.DataFrame(stabilization_list)\n",
    "\n",
    "# add info about brain area\n",
    "units_info_df_subset = units_info_df[['unit_id', 'ecephys_structure_acronym']].copy()\n",
    "\n",
    "tau_df.rename(columns={'signal':'unit_id'}, inplace=True)\n",
    "tau_df = pd.merge(tau_df, units_info_df_subset, on='unit_id', how='left')\n",
    "\n",
    "stabilization_df.rename(columns={'signal':'unit_id'}, inplace=True)\n",
    "stabilization_df = pd.merge(stabilization_df, units_info_df_subset, on='unit_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b13ac9-737d-4da5-98d7-770ed3aa8f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7565b198-c382-431b-bc7d-199dc803e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m_iterations_x_axes = [20, 40, 60, 80, 100, 150, 200, 500, 1000]\n",
    "# 2nd plot 95% CI\n",
    "m_iterations_x_axes = [50, 100, 200, 500, 1000]\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(10,3))\n",
    "\n",
    "sns.lineplot(ax=axes[0], data=tau_df, x=\"m_iterations\", hue='unit_id', y=\"tau_sem_median_ms\", palette=['slategray'], lw=0.5, legend=False)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "sns.lineplot(ax=axes[1], data=tau_df, x=\"m_iterations\", y=\"tau_sem_median_ms\", color='slategray', lw=0.5, legend=False)\n",
    "\n",
    "#sns.lineplot(ax=axes[2], data=tau_df, x=\"m_iterations\", y=\"tau_sem_median_ms\", hue='ecephys_structure_acronym', lw=0.5, legend=False)\n",
    "\n",
    "fig.suptitle(f'N spike trains: {len(stabilization_df)}', y=1.1)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xticks(m_iterations_x_axes)\n",
    "    ax.set_xticklabels(m_iterations_x_axes, rotation=45, fontsize=8)\n",
    "    ax.set_xlim([50, 1000])\n",
    "    ax.set_xlabel('Number of resampling iterations')\n",
    "    ax.set_ylabel('Intrinsic timescale SEM (ms)')\n",
    "    ax.axhline(y=25, lw=0.5, color='red')\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "if save_fig:\n",
    "    plt.savefig(fig_folder + 'n_iterations_vs_sem.png' , bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(fig_folder + 'n_iterations_vs_sem.svg' , bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966daea-fef5-4574-bf4a-aa2395791e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_signals = len(stabilization_df)\n",
    "n_signals_below_l = []\n",
    "for n_iterations in m_iterations_x_axes:\n",
    "    n_signals_below = len(stabilization_df.query('stabilization_point <= @n_iterations'))\n",
    "    n_signals_below_l.append(n_signals_below/n_signals * 100)\n",
    "\n",
    "stab_below_df = pd.DataFrame({\n",
    "    \"m_iterations\": m_iterations_x_axes,\n",
    "    \"n_signals_below_perc\": n_signals_below_l\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0dd4d-40b1-49e6-b838-7f91d9a33bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stab_below_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9161ec56-5f92-43e1-88ae-6d13a0297e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "stab_point_count_df = stabilization_df.groupby(by='stabilization_point', dropna=False, as_index=False).count()\n",
    "stab_point_count_df['stabilization_point'] = stab_point_count_df['stabilization_point'].fillna('no')\n",
    "stab_point_count_df['stabilization_point'] = pd.Categorical(stab_point_count_df['stabilization_point'], \n",
    "                                                            categories=['no', 50, 100, 500, 1000], ordered=True)\n",
    "stab_point_count_df_sorted = stab_point_count_df.sort_values(by='stabilization_point')\n",
    "stab_point_count_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef01f24e-9dac-4f62-8774-de8e635e40ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize=(10,3))\n",
    "\n",
    "sns.barplot(ax=axes[0], x='stabilization_point', y='unit_id', data=stab_point_count_df_sorted, color='slategray')\n",
    "axes[0].set_title('Number of resampling iterations \\nwith SEM <= 25ms')\n",
    "axes[0].set_ylabel('Number of spike trains')\n",
    "\n",
    "sns.barplot(ax=axes[1], x='m_iterations', y='n_signals_below_perc', data=stab_below_df, color='slategray')\n",
    "labels = [str(np.round(v, 2)) + '%' if v else '' for v in axes[1].containers[0].datavalues]\n",
    "axes[1].bar_label(axes[1].containers[0], labels=labels)\n",
    "axes[1].set_title('Percentage of spike trains \\nwith SEM <= 25ms')\n",
    "axes[1].set_ylabel('Percentage of spike trains')\n",
    "axes[1].set_ylim([0, 100])\n",
    "axes[1].grid(True, which=\"both\", axis='y', linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
    "\n",
    "fig.suptitle(f'N spike trains: {len(stabilization_df)}', y=1.1)\n",
    "\n",
    "for ax in axes.flat:\n",
    "     ax.set_xlabel('Number of resampling iterations')\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "if save_fig:\n",
    "    plt.savefig(fig_folder + 'stab_points.png' , bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(fig_folder + 'stab_points.svg' , bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc25c4f2-7563-45fd-8467-931c91eac6dc",
   "metadata": {},
   "source": [
    "##### Plot per brain area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732080be-c286-4bbe-8d95-1750b1f82d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area(area_name):\n",
    "    stabilization_df_area = stabilization_df.query('ecephys_structure_acronym == @area_name')\n",
    "    n_signals = len(stabilization_df_area)\n",
    "    n_signals_below_l = []\n",
    "    for n_iterations in m_iterations_x_axes:\n",
    "        n_signals_below = len(stabilization_df_area.query('stabilization_point <= @n_iterations'))\n",
    "        n_signals_below_l.append(n_signals_below/n_signals * 100)\n",
    "    \n",
    "    stab_below_df = pd.DataFrame({\n",
    "        \"m_iterations\": m_iterations_x_axes,\n",
    "        \"n_signals_below_perc\": n_signals_below_l\n",
    "    })\n",
    "    return stab_below_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743f4b7b-0502-491f-b8dc-207d3484d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = stabilization_df['ecephys_structure_acronym'].unique()\n",
    "print(areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c4552-a478-4a55-b157-e06b6b81cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stabilization_df_area = get_area(areas[1])\n",
    "stabilization_df_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08ba2b4-0163-4c6c-b434-1130afa5ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,4,figsize=(16,6))\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.6)\n",
    "\n",
    "for i in range(4):\n",
    "    stabilization_df_area = get_area(areas[i])\n",
    "    sns.barplot(ax=axes[0,i], x='m_iterations', y='n_signals_below_perc', data=stabilization_df_area, color='slategray')\n",
    "    labels = [str(np.round(v, 2)) + '%' if v else '' for v in axes[0,i].containers[0].datavalues]\n",
    "    axes[0,i].bar_label(axes[0, i].containers[0], labels=labels, fontsize=7)\n",
    "    axes[0,i].set_title(areas[i])\n",
    "\n",
    "for i in range(4, 8):\n",
    "    stabilization_df_area = get_area(areas[i])\n",
    "    sns.barplot(ax=axes[1,i-4], x='m_iterations', y='n_signals_below_perc', data=stabilization_df_area, color='slategray')\n",
    "    labels = [str(np.round(v, 2)) + '%' if v else '' for v in axes[1,i-4].containers[0].datavalues]\n",
    "    axes[1,i-4].bar_label(axes[1, i-4].containers[0], labels=labels, fontsize=7)\n",
    "    axes[1,i-4].set_title(areas[i])\n",
    "\n",
    "fig.suptitle(f'N spike trains: {len(stabilization_df)}', y=1)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('Number of resampling iterations')\n",
    "    ax.set_ylabel('Percentage of spike trains')\n",
    "    ax.set_ylim([0, 100])\n",
    "    ax.grid(True, which=\"both\", axis='y', linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "if save_fig:\n",
    "    plt.savefig(fig_folder + 'stab_points_per_area.png' , bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(fig_folder + 'stab_points_per_area.svg' , bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
