{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b1b6c08-9434-4f91-9117-5eb95ee10ad4",
   "metadata": {},
   "source": [
    "Prepare summary datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33aee8d9-673e-4302-b31b-e56b96b123e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from isttc.scripts.cfg_global import project_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca96473-a0d8-461a-be79-b2c2ef2d4662",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = project_folder_path + 'synthetic_dataset\\\\'\n",
    "results_folder = project_folder_path + 'results\\\\synthetic\\\\results\\\\param_fr_alpha_tau\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c3c84a-c3a3-4ebb-9c41-416a5787fc2e",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1362c68-e0b0-4d91-9715-660f2471c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acf_decline_flag(acf_, start_idx=1, end_idx=4):\n",
    "    acf_decay = np.all(np.diff(acf_[start_idx:end_idx]) <= 0)\n",
    "    return acf_decay\n",
    "\n",
    "def get_tau_df(acf_dict_, signal_len_, method_, alphas_, fr_values_, taus_ms_, bin_size_=50):\n",
    "    data = []\n",
    "    for unit_id, unit_data in acf_dict_.items():\n",
    "        taus = unit_data['taus']  \n",
    "        data.append({\n",
    "            'unit_id': unit_id,\n",
    "            'tau': taus['tau'],\n",
    "            'tau_lower': taus['tau_lower'],\n",
    "            'tau_upper': taus['tau_upper'],\n",
    "            'fit_r_squared': taus['fit_r_squared'],\n",
    "            'acf_decline': calculate_acf_decline_flag(unit_data['acf'], start_idx=1, end_idx=4)\n",
    "        })\n",
    "    tau_df = pd.DataFrame(data)\n",
    "    tau_df['method'] = method_\n",
    "    tau_df['tau_ms'] = tau_df['tau'] * bin_size_\n",
    "    tau_df['duration_s'] = signal_len_\n",
    "    tau_df['fr'] = fr_values_\n",
    "    tau_df['alpha'] = alphas_\n",
    "    tau_df['tau_ms_true'] = taus_ms_\n",
    "    tau_df['tau_diff_abs'] = np.abs(tau_df['tau_ms'] - tau_df['tau_ms_true'])\n",
    "    tau_df['tau_diff_rel'] = tau_df['tau_diff_abs'] / tau_df['tau_ms_true'] * 100\n",
    "    tau_df['ci_width'] = np.abs(tau_df['tau_upper'] - tau_df['tau_lower'])\n",
    "    \n",
    "    rows_with_nans_df = tau_df[tau_df.isna().any(axis=1)]\n",
    "    n_rows_with_nan = len(rows_with_nans_df)\n",
    "    print(f'N rows with NaNs {n_rows_with_nan}')\n",
    "    \n",
    "    return tau_df\n",
    "\n",
    "def get_trials_plot_df(trial_dict_, n_trials_, method_, alphas_, fr_values_, taus_ms_, n_iteration_=None, bin_size_=50):\n",
    "    records = []\n",
    "    for unit_id, data in trial_dict_.items():\n",
    "        taus = data['taus']\n",
    "        acfs = data['acf']\n",
    "\n",
    "        if n_iteration_ is not None:\n",
    "            # only one trial per unit\n",
    "            idx = n_iteration_[unit_id]\n",
    "            taus_to_iter = [(taus[idx], acfs[idx])]\n",
    "        else:\n",
    "            # all trials for this unit\n",
    "            taus_to_iter = zip(taus, acfs)\n",
    "\n",
    "        for tau_dict, acf_array in taus_to_iter:\n",
    "            records.append({\n",
    "                'unit_id': unit_id,\n",
    "                'tau': tau_dict['tau'],\n",
    "                'tau_lower': tau_dict['tau_lower'],\n",
    "                'tau_upper': tau_dict['tau_upper'],\n",
    "                'fit_r_squared': tau_dict['fit_r_squared'],\n",
    "                'acf_decline': calculate_acf_decline_flag(acf_array, start_idx=1, end_idx=4),\n",
    "                'method': method_,\n",
    "            })\n",
    "\n",
    "    tau_df = pd.DataFrame.from_records(records)\n",
    "    tau_df['tau_ms'] = tau_df['tau'] * bin_size_\n",
    "    tau_df['n_trials'] = n_trials_\n",
    "    tau_df['fr'] = fr_values_\n",
    "    tau_df['alpha'] = alphas_\n",
    "    tau_df['tau_ms_true'] = taus_ms_\n",
    "    tau_df['tau_diff_abs'] = np.abs(tau_df['tau_ms'] - tau_df['tau_ms_true'])\n",
    "    tau_df['tau_diff_rel'] = tau_df['tau_diff_abs'] / tau_df['tau_ms_true'] * 100\n",
    "    tau_df['ci_width'] = np.abs(tau_df['tau_upper'] - tau_df['tau_lower'])\n",
    "\n",
    "    nan_count = tau_df.isna().any(axis=1).sum()\n",
    "    if nan_count > 0:\n",
    "        print(f'N rows with NaNs {nan_count}')\n",
    "\n",
    "    return tau_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec52b4bc-f454-41cf-b175-39758c89e077",
   "metadata": {},
   "source": [
    "### Load spike trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def971cb-e6f7-44d5-8844-4ad3f2e842cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n spike trains 100000, len 598.8243581617338, duration_ms [600000 600000 600000 ... 600000 600000 600000]\n",
      "Lv loaded for n spike trains 100000\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_folder + 'spike_trains.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "spike_trains = data['spike_trains']\n",
    "alphas = data['alphas']\n",
    "fr_values = data['fr_values']\n",
    "taus_ms = data['tau_ms']\n",
    "duration_ms = data['duration_ms']\n",
    "\n",
    "print(f'n spike trains {len(spike_trains)}, len {spike_trains[0][-1]/1000}, duration_ms {duration_ms}')\n",
    "\n",
    "lv_df = pd.read_pickle(results_folder + 'lv_df.pkl')\n",
    "print(f'Lv loaded for n spike trains {len(lv_df)}')\n",
    "\n",
    "#fs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a76ba-bf17-493a-9076-fe96191ecec7",
   "metadata": {},
   "source": [
    "### ACF, iSTTC, PersonR, iSTTC trails (сoncat)\n",
    "\n",
    "The summary dataset includes only units where all 4 methods have valid (non-NaN) rows and fit_r_squared ≥ 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc42d3c0-c37b-4bdd-acdc-b4f2652141ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load per method\n",
    "with open(results_folder + 'tau_isttc_full_50ms_20lags_dict.pkl', \"rb\") as f:\n",
    "    isttc_full_dict = pickle.load(f)\n",
    "\n",
    "with open(results_folder + 'tau_acf_full_50ms_20lags_dict.pkl', \"rb\") as f:\n",
    "    acf_full_dict = pickle.load(f)\n",
    "\n",
    "with open(results_folder + 'tau_pearsonr_trial_50ms_20lags_dict.pkl', \"rb\") as f:\n",
    "    pearsonr_trial_avg_dict = pickle.load(f)\n",
    "\n",
    "with open(results_folder + 'tau_isttc_trial_concat_50ms_20lags_dict.pkl', \"rb\") as f:\n",
    "    sttc_trial_concat_dict = pickle.load(f)\n",
    "\n",
    "print(f'len pearsonr_trial_avg_dict {len(pearsonr_trial_avg_dict)}')\n",
    "print(f'len sttc_trial_concat_dict {len(sttc_trial_concat_dict)}')\n",
    "print(f'len isttc_full_dict {len(isttc_full_dict)}')\n",
    "print(f'len acf_full_dict {len(acf_full_dict)}')\n",
    "\n",
    "acf_tau_full_df = get_tau_df(acf_full_dict, 600, 'acf_full', alphas, fr_values, taus_ms)\n",
    "isttc_tau_full_df = get_tau_df(isttc_full_dict, 600, 'isttc_full', alphas, fr_values, taus_ms)\n",
    "random_trials_impl = np.zeros(len(sttc_trial_concat_dict)).astype(int)\n",
    "pearsontr_trial_avg_plot_df = get_trials_plot_df(pearsonr_trial_avg_dict, 40, 'pearsonr_trial_avg', alphas, fr_values, taus_ms, random_trials_impl)\n",
    "sttc_trial_concat_plot_df = get_trials_plot_df(sttc_trial_concat_dict, 40, 'sttc_trial_concat', alphas, fr_values, taus_ms, random_trials_impl)\n",
    "\n",
    "# prepare df\n",
    "tau_all_long_df = pd.concat([acf_tau_full_df, isttc_tau_full_df, pearsontr_trial_avg_plot_df, sttc_trial_concat_plot_df])\n",
    "tau_all_long_df.reset_index(inplace=True, drop=True)\n",
    "tau_all_long_df.drop(columns=['duration_s'], inplace=True)\n",
    "tau_all_long_df.drop(columns=['n_trials'], inplace=True)\n",
    "tau_all_long_df = tau_all_long_df.merge(lv_df[['unit_id', 'lv']].copy(), on='unit_id', how='left')\n",
    "print(f'len tau_all_long_df {len(tau_all_long_df)}')\n",
    "# leave only units with both methods (no NaNs and r_squared >= 0)\n",
    "required = {'acf_full', 'isttc_full', 'pearsonr_trial_avg', 'sttc_trial_concat'}\n",
    "tau_all_long_df_clean = (\n",
    "    tau_all_long_df.groupby(\"unit_id\")\n",
    "      .filter(lambda g: (\n",
    "          len(g) == 4\n",
    "          and set(g[\"method\"]) == required\n",
    "          and g.notna().all().all()\n",
    "          and (g[\"fit_r_squared\"] >= 0).all()\n",
    "      ))\n",
    ")\n",
    "tau_all_long_df_clean.reset_index(inplace=True, drop=True)\n",
    "print(f'len tau_all_long_df_clean {len(tau_all_long_df_clean)}, per method {len(tau_all_long_df_clean)/4}')\n",
    "\n",
    "#save df\n",
    "tau_all_long_df_clean.to_csv(results_folder + 'summary_tau_all_long_df.csv')\n",
    "tau_all_long_df_clean.to_pickle(results_folder + 'summary_tau_all_long_df.pkl')\n",
    "\n",
    "# save full df as well (with NaNs and negative r_squared)\n",
    "tau_all_long_df.to_csv(results_folder + 'summary_tau_all_long_df_all_units.csv')\n",
    "tau_all_long_df.to_pickle(results_folder + 'summary_tau_all_long_df_all_units.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a728431f-ae03-45a5-8b6d-2276eb26d88a",
   "metadata": {},
   "source": [
    "### ACF vs iSTTC, full signal\n",
    "\n",
    "The summary dataset includes only units where all 2 methods have valid (non-NaN) rows and fit_r_squared ≥ 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73168ac0-59c6-4e9d-942a-fbccfc92232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load per method\n",
    "with open(results_folder + 'tau_isttc_full_50ms_20lags_dict.pkl', \"rb\") as f:\n",
    "    isttc_full_dict = pickle.load(f)\n",
    "\n",
    "with open(results_folder + 'tau_acf_full_50ms_20lags_dict.pkl', \"rb\") as f:\n",
    "    acf_full_dict = pickle.load(f)\n",
    "\n",
    "print(f'len isttc_full_dict {len(isttc_full_dict)}')\n",
    "print(f'len acf_full_dict {len(acf_full_dict)}')\n",
    "\n",
    "acf_tau_full_df = get_tau_df(acf_full_dict, 600, 'acf_full', alphas, fr_values, taus_ms)\n",
    "isttc_tau_full_df = get_tau_df(isttc_full_dict, 600, 'isttc_full', alphas, fr_values, taus_ms)\n",
    "\n",
    "# prepare df\n",
    "tau_full_long_df = pd.concat([acf_tau_full_df, isttc_tau_full_df])\n",
    "tau_full_long_df.reset_index(inplace=True, drop=True)\n",
    "tau_full_long_df = tau_full_long_df.merge(lv_df[['unit_id', 'lv']].copy(), on='unit_id', how='left')\n",
    "print(f'len tau_full_long_df {len(tau_full_long_df)}')\n",
    "# leave only units with both methods (no NaNs and r_squared >= 0)\n",
    "required = {\"acf_full\", \"isttc_full\"}\n",
    "tau_full_long_df_clean = (\n",
    "    tau_full_long_df.groupby(\"unit_id\")\n",
    "      .filter(lambda g: (\n",
    "          len(g) == 2\n",
    "          and set(g[\"method\"]) == required\n",
    "          and g.notna().all().all()\n",
    "          and (g[\"fit_r_squared\"] >= 0).all()\n",
    "      ))\n",
    ")\n",
    "tau_full_long_df_clean.reset_index(inplace=True, drop=True)\n",
    "print(f'len tau_full_long_df_clean {len(tau_full_long_df_clean)}, per method {len(tau_full_long_df_clean)/2}')\n",
    "\n",
    "#save df\n",
    "tau_full_long_df_clean.to_csv(results_folder + 'summary_tau_full_long_df.csv')\n",
    "tau_full_long_df_clean.to_pickle(results_folder + 'summary_tau_full_long_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957c7826-6b49-4ab0-b00f-062ed53c3d61",
   "metadata": {},
   "source": [
    "### PearsonR vs iSTTC (concat and avg), trails\n",
    "\n",
    "The summary dataset includes only units where all 3 methods have valid (non-NaN) rows and fit_r_squared ≥ 0. STTC trail avg is not included in the paper figures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfa6cd-447d-47c7-93b9-41f43c20f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load per method\n",
    "with open(results_folder + 'tau_pearsonr_trial_50ms_20lags_dict.pkl', \"rb\") as f:\n",
    "    pearsonr_trial_avg_dict = pickle.load(f)\n",
    "\n",
    "with open(results_folder + 'tau_sttc_trial_avg_50ms_20lags_dict.pkl', \"rb\") as f:\n",
    "    sttc_trial_avg_dict = pickle.load(f)\n",
    "\n",
    "with open(results_folder + 'tau_isttc_trial_concat_50ms_20lags_dict.pkl', \"rb\") as f:\n",
    "    sttc_trial_concat_dict = pickle.load(f)\n",
    "\n",
    "print(f'len pearsonr_trial_avg_dict {len(pearsonr_trial_avg_dict)}')\n",
    "print(f'len sttc_trial_avg_dict {len(sttc_trial_avg_dict)}')\n",
    "print(f'len sttc_trial_concat_dict {len(sttc_trial_concat_dict)}')\n",
    "\n",
    "# for trial based measures one realization of trials is taken\n",
    "random_trials_impl = np.zeros(len(sttc_trial_concat_dict)).astype(int)\n",
    "\n",
    "pearsontr_trial_avg_plot_df = get_trials_plot_df(pearsonr_trial_avg_dict, 40, 'pearsonr_trial_avg', alphas, fr_values, taus_ms, random_trials_impl)\n",
    "sttc_trial_concat_plot_df = get_trials_plot_df(sttc_trial_concat_dict, 40, 'sttc_trial_concat', alphas, fr_values, taus_ms, random_trials_impl)\n",
    "sttc_trial_avg_plot_df = get_trials_plot_df(sttc_trial_avg_dict, 40, 'sttc_trial_avg', alphas, fr_values, taus_ms, random_trials_impl)\n",
    "\n",
    "# prepare df\n",
    "tau_trials_long_df = pd.concat([pearsontr_trial_avg_plot_df, sttc_trial_concat_plot_df, sttc_trial_avg_plot_df])\n",
    "tau_trials_long_df.reset_index(inplace=True, drop=True)\n",
    "tau_trials_long_df = tau_trials_long_df.merge(lv_df[['unit_id', 'lv']].copy(), on='unit_id', how='left')\n",
    "print(f'len tau_trials_long_df {len(tau_trials_long_df)}')\n",
    "# leave only units with both methods (no NaNs and r_squared >= 0)\n",
    "required = {'pearsonr_trial_avg', 'sttc_trial_concat', 'sttc_trial_avg'}\n",
    "tau_trials_long_df_clean = (\n",
    "    tau_trials_long_df.groupby(\"unit_id\")\n",
    "      .filter(lambda g: (\n",
    "          len(g) == 3\n",
    "          and set(g[\"method\"]) == required\n",
    "          and g.notna().all().all()\n",
    "          and (g[\"fit_r_squared\"] >= 0).all()\n",
    "      ))\n",
    ")\n",
    "tau_trials_long_df_clean.reset_index(inplace=True, drop=True)\n",
    "print(f'len tau_trials_long_df_clean {len(tau_trials_long_df_clean)}, per method {len(tau_trials_long_df_clean)/3}')\n",
    "\n",
    "#save df\n",
    "tau_trials_long_df_clean.to_csv(results_folder + 'summary_tau_trials_long_df.csv')\n",
    "tau_trials_long_df_clean.to_pickle(results_folder + 'summary_tau_trials_long_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e9c645-c488-4764-8a7c-57c4726b93db",
   "metadata": {},
   "source": [
    "### ACF vs iSTTC, varying signal length (60, 150, 300, 450, 600 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e24124-fe7f-48b3-9b65-5f4de7b2e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60 sec\n",
    "with open(results_folder + 'tau_acf_full_50ms_20lags_len_60sec_dict.pkl', \"rb\") as f:\n",
    "    acf_60_dict = pickle.load(f)\n",
    "with open(results_folder + 'tau_isttc_full_50ms_20lags_len_60sec_dict.pkl', \"rb\") as f:\n",
    "    sttc_60_dict = pickle.load(f)\n",
    "\n",
    "# 150 sec\n",
    "with open(results_folder + 'tau_acf_full_50ms_20lags_len_150sec_dict.pkl', \"rb\") as f:\n",
    "    acf_150_dict = pickle.load(f)\n",
    "with open(results_folder + 'tau_isttc_full_50ms_20lags_len_150sec_dict.pkl', \"rb\") as f:\n",
    "    sttc_150_dict = pickle.load(f)\n",
    "\n",
    "# 300 sec\n",
    "with open(results_folder + 'tau_acf_full_50ms_20lags_len_300sec_dict.pkl', \"rb\") as f:\n",
    "    acf_300_dict = pickle.load(f)\n",
    "with open(results_folder + 'tau_isttc_full_50ms_20lags_len_300sec_dict.pkl', \"rb\") as f:\n",
    "    sttc_300_dict = pickle.load(f)\n",
    "\n",
    "# 450 sec\n",
    "with open(results_folder + 'tau_acf_full_50ms_20lags_len_450sec_dict.pkl', \"rb\") as f:\n",
    "    acf_450_dict = pickle.load(f)\n",
    "with open(results_folder + 'tau_isttc_full_50ms_20lags_len_450sec_dict.pkl', \"rb\") as f:\n",
    "    sttc_450_dict = pickle.load(f)\n",
    "\n",
    "# 600 sec\n",
    "with open(results_folder + 'tau_acf_full_50ms_20lags_dict.pkl', \"rb\") as f:\n",
    "    acf_600_dict = pickle.load(f)\n",
    "with open(results_folder + 'tau_isttc_full_50ms_20lags_dict.pkl', \"rb\") as f:\n",
    "    sttc_600_dict = pickle.load(f)\n",
    "\n",
    "print(f'len acf_60_dict {len(acf_60_dict)}')\n",
    "print(f'len sttc_60_dict {len(sttc_60_dict)}')\n",
    "\n",
    "print(f'len acf_150_dict {len(acf_150_dict)}')\n",
    "print(f'len sttc_150_dict {len(sttc_150_dict)}')\n",
    "\n",
    "print(f'len acf_300_dict {len(acf_300_dict)}')\n",
    "print(f'len sttc_300_dict {len(sttc_300_dict)}')\n",
    "\n",
    "print(f'len acf_450_dict {len(acf_450_dict)}')\n",
    "print(f'len sttc_450_dict {len(sttc_450_dict)}')\n",
    "\n",
    "print(f'len acf_600_dict {len(acf_600_dict)}')\n",
    "print(f'len sttc_600_dict {len(sttc_600_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e4d31-a9a4-4683-8841-1f5dc8089e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "acf_60_df = get_tau_df(acf_60_dict, 60, 'acf_full', alphas, fr_values, taus_ms)\n",
    "acf_150_df = get_tau_df(acf_150_dict, 150, 'acf_full', alphas, fr_values, taus_ms)\n",
    "acf_300_df = get_tau_df(acf_300_dict, 300, 'acf_full', alphas, fr_values, taus_ms)\n",
    "acf_450_df = get_tau_df(acf_450_dict, 450, 'acf_full', alphas, fr_values, taus_ms)\n",
    "acf_600_df = get_tau_df(acf_600_dict, 600, 'acf_full', alphas, fr_values, taus_ms)\n",
    "\n",
    "isttc_60_df = get_tau_df(sttc_60_dict, 60, 'isttc_full', alphas, fr_values, taus_ms)\n",
    "isttc_150_df = get_tau_df(sttc_150_dict, 150, 'isttc_full', alphas, fr_values, taus_ms)\n",
    "isttc_300_df = get_tau_df(sttc_300_dict, 300, 'isttc_full', alphas, fr_values, taus_ms)\n",
    "isttc_450_df = get_tau_df(sttc_450_dict, 450, 'isttc_full', alphas, fr_values, taus_ms)\n",
    "isttc_600_df = get_tau_df(sttc_600_dict, 600, 'isttc_full', alphas, fr_values, taus_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d1da7d-cd1d-4782-9b48-4c8a7896af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pair(acf_df, isttc_df, id_col=\"unit_id\", r2_col=\"fit_r_squared\", label=None):\n",
    "    acf_nans_units   = set(acf_df.loc[acf_df.isna().any(axis=1), id_col])\n",
    "    isttc_nans_units = set(isttc_df.loc[isttc_df.isna().any(axis=1), id_col])\n",
    "    neg_acf_units    = set(acf_df.loc[acf_df[r2_col] < 0, id_col])\n",
    "    neg_isttc_units  = set(isttc_df.loc[isttc_df[r2_col] < 0, id_col])\n",
    "\n",
    "    drop_units = acf_nans_units | isttc_nans_units | neg_acf_units | neg_isttc_units\n",
    "\n",
    "    acf_clean   = acf_df[~acf_df[id_col].isin(drop_units)].copy()\n",
    "    isttc_clean = isttc_df[~isttc_df[id_col].isin(drop_units)].copy()\n",
    "\n",
    "    stats = {\n",
    "        \"label\": label,\n",
    "        \"acf_nans\": len(acf_nans_units),\n",
    "        \"isttc_nans\": len(isttc_nans_units),\n",
    "        \"neg_acf\": len(neg_acf_units),\n",
    "        \"neg_isttc\": len(neg_isttc_units),\n",
    "        \"dropped_units_total\": len(drop_units),\n",
    "        \"acf_len\": len(acf_clean),\n",
    "        \"isttc_len\": len(isttc_clean),\n",
    "    }\n",
    "    return acf_clean, isttc_clean, stats\n",
    "\n",
    "pairs = {\n",
    "    60:  (acf_60_df,  isttc_60_df),\n",
    "    150: (acf_150_df, isttc_150_df),\n",
    "    300: (acf_300_df, isttc_300_df),\n",
    "    450: (acf_450_df, isttc_450_df),\n",
    "    600: (acf_600_df, isttc_600_df),\n",
    "}\n",
    "\n",
    "cleaned = {}\n",
    "summary_rows = []\n",
    "for win, (acf_df, ist_df) in pairs.items():\n",
    "    acf_c, ist_c, stats = clean_pair(acf_df, ist_df, label=f\"{win} ms\")\n",
    "    cleaned[win] = (acf_c, ist_c)\n",
    "    summary_rows.append(stats)\n",
    "\n",
    "# sanity check\n",
    "summary_df = pd.DataFrame(summary_rows).set_index(\"label\")\n",
    "print(summary_df)\n",
    "\n",
    "tau_full_long_var_length_df_clean = pd.concat(\n",
    "    [df for win in sorted(cleaned) for df in cleaned[win]],\n",
    "    ignore_index=True\n",
    ")\n",
    "print(f\"len tau_full_long_var_length_df_clean {len(tau_full_long_var_length_df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1bf853-f4ff-4836-ac2b-17badfe6c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save df\n",
    "tau_full_long_var_length_df_clean.to_csv(results_folder + 'summary_tau_full_long_var_length_df.csv')\n",
    "tau_full_long_var_length_df_clean.to_pickle(results_folder + 'summary_tau_full_long_var_length_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e923c8d-fff2-4f5d-8756-232410134bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_full_long_var_length_df = pd.concat(\n",
    "    [df for _, (acf_df, isttc_df) in sorted(pairs.items()) for df in (acf_df, isttc_df)],\n",
    "    ignore_index=True\n",
    ")\n",
    "print(f\"len tau_full_long_var_length_df {len(tau_full_long_var_length_df)}\")\n",
    "\n",
    "#save df\n",
    "tau_full_long_var_length_df.to_csv(results_folder + 'summary_tau_full_long_var_length_df_all_units.csv')\n",
    "tau_full_long_var_length_df.to_pickle(results_folder + 'summary_tau_full_long_var_length_df_all_units.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b7671-a82e-4dca-9b61-a98830a2f268",
   "metadata": {},
   "source": [
    "### PearsonR vs iSTTC (concat), varying number of trials (40, 60, 80, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f26f877-f2b3-487a-be65-7f8fc4eb8b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40 trials\n",
    "with open(results_folder + 'tau_pearsonr_trial_50ms_20lags_dict.pkl', \"rb\") as f:\n",
    "    pearsonr_trial_avg_40_dict = pickle.load(f)\n",
    "with open(results_folder + 'tau_isttc_trial_concat_50ms_20lags_dict.pkl', \"rb\") as f:\n",
    "    sttc_trial_concat_40_dict = pickle.load(f)\n",
    "\n",
    "# 60 trials\n",
    "with open(results_folder + 'tau_pearsonr_trial_50ms_20_lags_60_trials_dict.pkl', \"rb\") as f:\n",
    "    pearsonr_trial_avg_60_dict = pickle.load(f)\n",
    "with open(results_folder + 'tau_isttc_trial_concat_50ms_20lags_60_trials_dict.pkl', \"rb\") as f:\n",
    "    sttc_trial_concat_60_dict = pickle.load(f)\n",
    "\n",
    "# 80 trials\n",
    "with open(results_folder + 'tau_pearsonr_trial_50ms_20_lags_80_trials_dict.pkl', \"rb\") as f:\n",
    "    pearsonr_trial_avg_80_dict = pickle.load(f)\n",
    "with open(results_folder + 'tau_isttc_trial_concat_50ms_20lags_80_trials_dict.pkl', \"rb\") as f:\n",
    "    sttc_trial_concat_80_dict = pickle.load(f)\n",
    "\n",
    "# 100 trials\n",
    "with open(results_folder + 'tau_pearsonr_trial_50ms_20_lags_100_trials_dict.pkl', \"rb\") as f:\n",
    "    pearsonr_trial_avg_100_dict = pickle.load(f)\n",
    "with open(results_folder + 'tau_isttc_trial_concat_50ms_20lags_100_trials_dict.pkl', \"rb\") as f:\n",
    "    sttc_trial_concat_100_dict = pickle.load(f)\n",
    "\n",
    "print(f'len pearsonr_trial_avg_40_dict {len(pearsonr_trial_avg_40_dict)}')\n",
    "print(f'len sttc_trial_concat_40_dict {len(sttc_trial_concat_40_dict)}')\n",
    "\n",
    "print(f'len pearsonr_trial_avg_60_dict {len(pearsonr_trial_avg_60_dict)}')\n",
    "print(f'len sttc_trial_concat_60_dict {len(sttc_trial_concat_60_dict)}')\n",
    "\n",
    "print(f'len pearsonr_trial_avg_80_dict {len(pearsonr_trial_avg_80_dict)}')\n",
    "print(f'len sttc_trial_concat_80_dict {len(sttc_trial_concat_80_dict)}')\n",
    "\n",
    "print(f'len pearsonr_trial_avg_100_dict {len(pearsonr_trial_avg_100_dict)}')\n",
    "print(f'len sttc_trial_concat_100_dict {len(sttc_trial_concat_100_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee5770-c15b-48e5-b9a8-6d558a7ffade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for trial based measures one realization of trials is taken\n",
    "random_trials_impl = np.zeros(len(pearsonr_trial_avg_40_dict)).astype(int)\n",
    "\n",
    "pearsontr_trial_avg_40_df = get_trials_plot_df(pearsonr_trial_avg_40_dict, 40, 'pearsonr_trial_avg', alphas, fr_values, taus_ms, random_trials_impl)\n",
    "pearsontr_trial_avg_60_df = get_trials_plot_df(pearsonr_trial_avg_60_dict, 60, 'pearsonr_trial_avg', alphas, fr_values, taus_ms, random_trials_impl)\n",
    "pearsontr_trial_avg_80_df = get_trials_plot_df(pearsonr_trial_avg_80_dict, 80, 'pearsonr_trial_avg', alphas, fr_values, taus_ms, random_trials_impl)\n",
    "pearsontr_trial_avg_100_df = get_trials_plot_df(pearsonr_trial_avg_100_dict, 100, 'pearsonr_trial_avg', alphas, fr_values, taus_ms, random_trials_impl)\n",
    "\n",
    "sttc_trial_concat_40_df = get_trials_plot_df(sttc_trial_concat_40_dict, 40, 'sttc_trial_concat', alphas, fr_values, taus_ms, random_trials_impl)\n",
    "sttc_trial_concat_60_df = get_trials_plot_df(sttc_trial_concat_60_dict, 60, 'sttc_trial_concat', alphas, fr_values, taus_ms, random_trials_impl)\n",
    "sttc_trial_concat_80_df = get_trials_plot_df(sttc_trial_concat_80_dict, 80, 'sttc_trial_concat', alphas, fr_values, taus_ms, random_trials_impl)\n",
    "sttc_trial_concat_100_df = get_trials_plot_df(sttc_trial_concat_100_dict, 100, 'sttc_trial_concat', alphas, fr_values, taus_ms, random_trials_impl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28079689-b61e-42fc-a689-b38b3b3ce3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = {\n",
    "    40:  (pearsontr_trial_avg_40_df,  sttc_trial_concat_40_df),\n",
    "    60: (pearsontr_trial_avg_60_df, sttc_trial_concat_60_df),\n",
    "    80: (pearsontr_trial_avg_80_df, sttc_trial_concat_80_df),\n",
    "    100: (pearsontr_trial_avg_100_df, sttc_trial_concat_100_df),\n",
    "}\n",
    "\n",
    "cleaned = {}\n",
    "summary_rows = []\n",
    "for win, (acf_df, ist_df) in pairs.items():\n",
    "    acf_c, ist_c, stats = clean_pair(acf_df, ist_df, label=f\"{win} ms\")\n",
    "    cleaned[win] = (acf_c, ist_c)\n",
    "    summary_rows.append(stats)\n",
    "\n",
    "# sanity check\n",
    "summary_df = pd.DataFrame(summary_rows).set_index(\"label\")\n",
    "print(summary_df)\n",
    "\n",
    "tau_trials_long_var_num_trials_df_clean = pd.concat(\n",
    "    [df for win in sorted(cleaned) for df in cleaned[win]],\n",
    "    ignore_index=True\n",
    ")\n",
    "print(f\"len tau_trials_long_var_num_trials_df_clean {len(tau_trials_long_var_num_trials_df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65f248f-008d-4d07-961e-e80a93cda757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save df\n",
    "tau_trials_long_var_num_trials_df_clean.to_csv(results_folder + 'summary_tau_trials_long_var_num_trials_df.csv')\n",
    "tau_trials_long_var_num_trials_df_clean.to_pickle(results_folder + 'summary_tau_trials_long_var_num_trials_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603219b8-aa5e-44ff-b05f-3f2a5d0d7591",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_trials_long_var_num_trials_df = pd.concat(\n",
    "    [df for _, (acf_df, isttc_df) in sorted(pairs.items()) for df in (acf_df, isttc_df)],\n",
    "    ignore_index=True\n",
    ")\n",
    "print(f\"len tau_trials_long_var_num_trials_df {len(tau_trials_long_var_num_trials_df)}\")\n",
    "\n",
    "#save df\n",
    "tau_trials_long_var_num_trials_df.to_csv(results_folder + 'tau_trials_long_var_num_trials_df_all_units.csv')\n",
    "tau_trials_long_var_num_trials_df.to_pickle(results_folder + 'tau_trials_long_var_num_trials_df_all_units.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eda1f9-e8e0-459c-a3e2-0d0f0666652a",
   "metadata": {},
   "source": [
    "### ACF and iSTTC full signal, different number of lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd9728dc-28a1-4bdf-8fb5-84fab394810b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len acf_full_dict 50ms 100000\n",
      "N rows with NaNs 11\n",
      "len acf_full_dict 100ms 100000\n",
      "N rows with NaNs 11\n",
      "len acf_full_dict 10ms 100000\n",
      "N rows with NaNs 11\n",
      "len acf_full_dict 40ms 100000\n",
      "N rows with NaNs 11\n",
      "len acf_full_dict 60ms 100000\n",
      "N rows with NaNs 11\n",
      "len tau_full_long_df 500000\n",
      "len tau_full_long_df_clean 492235, per method 98447.0\n"
     ]
    }
   ],
   "source": [
    "results_folder_lags = project_folder_path + 'results\\\\synthetic\\\\results\\\\bin_size_runs\\\\full_signal\\\\'\n",
    "\n",
    "acf_configs = [\n",
    "    {\n",
    "        \"path\": results_folder,  # assumes this is defined earlier\n",
    "        \"filename\": \"tau_acf_full_50ms_20lags_dict.pkl\",\n",
    "        \"n_lags\": 20,\n",
    "        \"bin_size\": 50,       \n",
    "    },\n",
    "    {\n",
    "        \"path\": results_folder_lags,\n",
    "        \"filename\": \"acf_full_100ms_10lags_dict.pkl\",\n",
    "        \"n_lags\": 10,\n",
    "        \"bin_size\": 100,\n",
    "    },\n",
    "    {\n",
    "        \"path\": results_folder_lags,\n",
    "        \"filename\": \"acf_full_10ms_100lags_dict.pkl\",\n",
    "        \"n_lags\": 100,\n",
    "        \"bin_size\": 10,\n",
    "    },\n",
    "    {\n",
    "        \"path\": results_folder_lags,\n",
    "        \"filename\": \"acf_full_40ms_25lags_dict.pkl\",\n",
    "        \"n_lags\": 25,\n",
    "        \"bin_size\": 40,\n",
    "    },\n",
    "    {\n",
    "        \"path\": results_folder_lags,\n",
    "        \"filename\": \"acf_full_60ms_16lags_dict.pkl\",\n",
    "        \"n_lags\": 16,\n",
    "        \"bin_size\": 60,\n",
    "    },\n",
    "]\n",
    "\n",
    "tau_dfs = []\n",
    "for cfg in acf_configs:\n",
    "    full_path = cfg[\"path\"] + cfg[\"filename\"]\n",
    "    with open(full_path, \"rb\") as f:\n",
    "        acf_dict = pickle.load(f)\n",
    "\n",
    "    bin_size = cfg[\"bin_size\"]\n",
    "    print(f'len acf_full_dict {bin_size}ms {len(acf_dict)}')\n",
    "    tau_df = get_tau_df(acf_dict, 600, 'acf_full', alphas, fr_values, taus_ms, bin_size)\n",
    "    tau_df[\"n_lags\"] = cfg[\"n_lags\"]\n",
    "    tau_df[\"bin_size\"] = bin_size\n",
    "    tau_dfs.append(tau_df)\n",
    "\n",
    "# make long df\n",
    "tau_full_long_df = pd.concat(tau_dfs, ignore_index=True)\n",
    "tau_full_long_df = tau_full_long_df.merge(lv_df[[\"unit_id\", \"lv\"]].copy(),\n",
    "                                          on=\"unit_id\", how=\"left\")\n",
    "print(f'len tau_full_long_df {len(tau_full_long_df)}')\n",
    "\n",
    "# leave only units with both methods (no NaNs and r_squared >= 0)\n",
    "required = {cfg[\"bin_size\"] for cfg in acf_configs}\n",
    "tau_full_long_df_clean = (\n",
    "    tau_full_long_df.groupby(\"unit_id\")\n",
    "    .filter(\n",
    "        lambda g: (\n",
    "            len(g) == len(required)\n",
    "            and set(g[\"bin_size\"]) == required\n",
    "            and g.notna().all().all()\n",
    "            and (g[\"fit_r_squared\"] >= 0).all()\n",
    "        )\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\n",
    "    f'len tau_full_long_df_clean {len(tau_full_long_df_clean)}, '\n",
    "    f'per method {len(tau_full_long_df_clean) / len(required)}'\n",
    ")\n",
    "\n",
    "# save df\n",
    "tau_full_long_df_clean.to_csv(results_folder_lags + 'summary_tau_full_long_acf_lags_df.csv')\n",
    "tau_full_long_df_clean.to_pickle(results_folder_lags + 'summary_tau_full_long_acf_lags_df.pkl')\n",
    "\n",
    "tau_full_long_df.to_csv(results_folder_lags + 'summary_tau_full_long_acf_lags_df_all_units.csv')\n",
    "tau_full_long_df.to_pickle(results_folder_lags + 'summary_tau_full_long_acf_lags_df_all_units.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50c1d06e-607f-43db-b28d-a0c07efe2497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len acf_full_dict 50ms 100000\n",
      "N rows with NaNs 0\n",
      "len acf_full_dict 100ms 100000\n",
      "N rows with NaNs 0\n",
      "len acf_full_dict 10ms 100000\n",
      "N rows with NaNs 0\n",
      "len acf_full_dict 40ms 100000\n",
      "N rows with NaNs 0\n",
      "len acf_full_dict 60ms 100000\n",
      "N rows with NaNs 0\n",
      "len tau_full_long_df 500000\n",
      "len tau_full_long_df_clean 492830, per method 98566.0\n"
     ]
    }
   ],
   "source": [
    "results_folder_lags = project_folder_path + 'results\\\\synthetic\\\\results\\\\bin_size_runs\\\\full_signal\\\\'\n",
    "\n",
    "acf_configs = [\n",
    "    {\n",
    "        \"path\": results_folder,  # assumes this is defined earlier\n",
    "        \"filename\": \"tau_isttc_full_50ms_20lags_dict.pkl\",\n",
    "        \"n_lags\": 20,\n",
    "        \"bin_size\": 50,       \n",
    "    },\n",
    "    {\n",
    "        \"path\": results_folder_lags,\n",
    "        \"filename\": \"acf_isttc_full_100ms_10lags_dict.pkl\",\n",
    "        \"n_lags\": 10,\n",
    "        \"bin_size\": 100,\n",
    "    },\n",
    "    {\n",
    "        \"path\": results_folder_lags,\n",
    "        \"filename\": \"acf_isttc_full_10ms_100lags_dict.pkl\",\n",
    "        \"n_lags\": 100,\n",
    "        \"bin_size\": 10,\n",
    "    },\n",
    "    {\n",
    "        \"path\": results_folder_lags,\n",
    "        \"filename\": \"acf_isttc_full_40ms_25lags_dict.pkl\",\n",
    "        \"n_lags\": 25,\n",
    "        \"bin_size\": 40,\n",
    "    },\n",
    "    {\n",
    "        \"path\": results_folder_lags,\n",
    "        \"filename\": \"acf_isttc_full_60ms_16lags_dict.pkl\",\n",
    "        \"n_lags\": 16,\n",
    "        \"bin_size\": 60,\n",
    "    },\n",
    "]\n",
    "\n",
    "tau_dfs = []\n",
    "for cfg in acf_configs:\n",
    "    full_path = cfg[\"path\"] + cfg[\"filename\"]\n",
    "    with open(full_path, \"rb\") as f:\n",
    "        acf_dict = pickle.load(f)\n",
    "\n",
    "    bin_size = cfg[\"bin_size\"]\n",
    "    print(f'len acf_full_dict {bin_size}ms {len(acf_dict)}')\n",
    "    tau_df = get_tau_df(acf_dict, 600, 'acf_isttc_full', alphas, fr_values, taus_ms, bin_size)\n",
    "    tau_df[\"n_lags\"] = cfg[\"n_lags\"]\n",
    "    tau_df[\"bin_size\"] = bin_size\n",
    "    tau_dfs.append(tau_df)\n",
    "\n",
    "# make long df\n",
    "tau_full_long_df = pd.concat(tau_dfs, ignore_index=True)\n",
    "tau_full_long_df = tau_full_long_df.merge(lv_df[[\"unit_id\", \"lv\"]].copy(),\n",
    "                                          on=\"unit_id\", how=\"left\")\n",
    "print(f'len tau_full_long_df {len(tau_full_long_df)}')\n",
    "\n",
    "# leave only units with both methods (no NaNs and r_squared >= 0)\n",
    "required = {cfg[\"bin_size\"] for cfg in acf_configs}\n",
    "tau_full_long_df_clean = (\n",
    "    tau_full_long_df.groupby(\"unit_id\")\n",
    "    .filter(\n",
    "        lambda g: (\n",
    "            len(g) == len(required)\n",
    "            and set(g[\"bin_size\"]) == required\n",
    "            and g.notna().all().all()\n",
    "            and (g[\"fit_r_squared\"] >= 0).all()\n",
    "        )\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\n",
    "    f'len tau_full_long_df_clean {len(tau_full_long_df_clean)}, '\n",
    "    f'per method {len(tau_full_long_df_clean) / len(required)}'\n",
    ")\n",
    "\n",
    "# save df\n",
    "tau_full_long_df_clean.to_csv(results_folder_lags + 'summary_tau_full_long_acf_isttc_lags_df.csv')\n",
    "tau_full_long_df_clean.to_pickle(results_folder_lags + 'summary_tau_full_long_acf_isttc_lags_df.pkl')\n",
    "\n",
    "tau_full_long_df.to_csv(results_folder_lags + 'summary_tau_full_long_acf_isttc_lags_df_all_units.csv')\n",
    "tau_full_long_df.to_pickle(results_folder_lags + 'summary_tau_full_long_acf_isttc_lags_df_all_units.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
