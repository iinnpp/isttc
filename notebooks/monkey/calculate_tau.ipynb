{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f51a4e-eb0d-4189-ae6d-21d8d085b316",
   "metadata": {},
   "source": [
    "Calculate taus:\n",
    "\n",
    "on unit level:\n",
    "1. Pearsonr trial average\n",
    "2. STTC trial average\n",
    "3. STTC trial concat\n",
    "   \n",
    "on trial level (not calculated yet):\n",
    "1. Pearsonr per trial\n",
    "2. ACF proper per trial\n",
    "3. iSTTC per trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "264f77cb-1fa3-41c5-adc5-336c1d18c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import from scripts\n",
    "import os\n",
    "current_wd = os.getcwd()\n",
    "os.chdir(os.path.abspath(\"..\\\\..\\\\..\\\\isttc\\\\scripts\"))\n",
    "from calculate_tau import fit_single_exp, func_single_exp_monkey\n",
    "from cfg_global import project_folder_path\n",
    "os.chdir(current_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e172328e-a498-43a0-acec-db8b553551a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_suffix = 'no'\n",
    "data_folder = project_folder_path + 'results\\\\monkey\\\\fixation_period_1000ms_' + empty_suffix + '_empty\\\\'\n",
    "results_folder = project_folder_path + 'results\\\\monkey\\\\fixation_period_1000ms_' + empty_suffix + '_empty\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddf708d-0bcb-4354-b331-813d854b8500",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f824a74c-0158-404b-a864-b4d3cab41bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tau_per_unit(acf_df_, acf_cols_, start_idx_):\n",
    "    \"\"\"\n",
    "    Calculate tau values per unit using single exponential fitting.\n",
    "\n",
    "    Parameters:\n",
    "    - acf_df (pd.DataFrame): DataFrame containing autocorrelation function (ACF) values.\n",
    "    - acf_cols (list): List of column names containing ACF data.\n",
    "    - start_idx (int): Starting index for fitting.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with fitting parameters and tau values per unit.\n",
    "    \"\"\"\n",
    "    acf_2d = acf_df_[acf_cols_].values.astype(np.float64)\n",
    "    n_units = acf_2d.shape[0]\n",
    "    print(f'Calculating taus for {acf_2d.shape}')\n",
    "    \n",
    "    results = [fit_single_exp(acf_2d[i, :], start_idx_, func_single_exp_monkey) for i in range(n_units)]\n",
    "\n",
    "    fit_popt_a, fit_popt_b, fit_popt_c, fit_tau, fit_r_squared, fit_log_message = zip(*[\n",
    "        (popt[0] if isinstance(popt, np.ndarray) else np.nan,\n",
    "         popt[1] if isinstance(popt, np.ndarray) else np.nan,\n",
    "         popt[2] if isinstance(popt, np.ndarray) else np.nan,\n",
    "         tau, r_squared, log_msg)\n",
    "        for popt, _, tau, r_squared, log_msg in results\n",
    "    ])\n",
    "\n",
    "    tau_df = pd.DataFrame({\n",
    "        'unit_id': acf_df_['unit_id'].values,\n",
    "        'fit_a': fit_popt_a,\n",
    "        'fit_b': fit_popt_b,\n",
    "        'fit_c': fit_popt_c,\n",
    "        'tau': fit_tau,\n",
    "        'r_squared': fit_r_squared,\n",
    "        'log_message': fit_log_message\n",
    "    })\n",
    "\n",
    "    float_cols = ['fit_a', 'fit_b', 'fit_c', 'tau', 'r_squared']\n",
    "    tau_df[float_cols] = tau_df[float_cols].astype(float)\n",
    "    \n",
    "    return tau_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1494b-0434-480e-977a-458b4ac9c5bd",
   "metadata": {},
   "source": [
    "### Calculate tau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d2f02-5b7c-4890-ad37-f713ae93d5ba",
   "metadata": {},
   "source": [
    "#### One tau per unit\n",
    "\n",
    "3 methods of doing that:\n",
    "* Pearsonr trial average (as in the paper)\n",
    "* STTC trial average\n",
    "* STTC trial concat\n",
    "\n",
    "todo: take ACF average over trails for a unit and fit? Or fit all ACFs with one function? (this requires calculating ACF per trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "545844af-2a85-4a85-8df2-e61c0b65bf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acf_cols ['acf_0', 'acf_1', 'acf_2', 'acf_3', 'acf_4', 'acf_5', 'acf_6', 'acf_7', 'acf_8', 'acf_9', 'acf_10', 'acf_11', 'acf_12', 'acf_13', 'acf_14', 'acf_15', 'acf_16', 'acf_17', 'acf_18', 'acf_19']\n"
     ]
    }
   ],
   "source": [
    "n_lags = 20\n",
    "bin_size = 50\n",
    "acf_cols = ['acf_' + str(i) for i in range(n_lags)]\n",
    "print('acf_cols {}'.format(acf_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cccd6aff-eea4-4941-b145-f2a4e60c82fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating taus for (539, 20)\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "ValueError: array must not contain infs or NaNs\n",
      "Possible reason: acf contains NaNs, low spike count\n",
      "Calculating taus for (539, 20)\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "Calculating taus for (539, 20)\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n",
      "OptimizeWarning: Covariance of the parameters could not be estimated\n"
     ]
    }
   ],
   "source": [
    "area = 'pfdl' \n",
    "\n",
    "# pearsonr trial avg\n",
    "pearsonr_trial_avg_df = pd.read_pickle(results_folder + 'binned\\\\' + area + '\\\\acf\\\\acf_pearsonr_trial_avg_1000ms_' + empty_suffix + '_empty_50ms_20lags_df.pkl')\n",
    "tau_pearsonr_trial_avg_df = calc_tau_per_unit(pearsonr_trial_avg_df, acf_cols, start_idx_=1)\n",
    "tau_pearsonr_trial_avg_df['tau_ms'] = tau_pearsonr_trial_avg_df['tau'] * bin_size\n",
    "tau_pearsonr_trial_avg_df.to_pickle(results_folder + 'binned\\\\' + area + '\\\\taus\\\\tau_pearsonr_trial_avg_1000ms_' + empty_suffix + '_empty_50ms_20lags_df.pkl')\n",
    "\n",
    "# sttc trial avg\n",
    "sttc_trial_avg_df = pd.read_pickle(results_folder + 'non_binned\\\\' + area + '\\\\acf\\\\acf_sttc_trial_avg_1000ms_' + empty_suffix + '_empty_50ms_20lags_df.pkl')\n",
    "tau_sttc_trial_avg_df = calc_tau_per_unit(sttc_trial_avg_df, acf_cols, start_idx_=1)\n",
    "tau_sttc_trial_avg_df['tau_ms'] = tau_sttc_trial_avg_df['tau'] * bin_size\n",
    "tau_sttc_trial_avg_df.to_pickle(results_folder + 'non_binned\\\\' + area + '\\\\taus\\\\tau_sttc_trial_avg_1000ms_' + empty_suffix + '_empty_50ms_20lags_df.pkl')\n",
    "\n",
    "# sttc trial concat\n",
    "sttc_trial_concat_df = pd.read_pickle(results_folder + 'non_binned\\\\' + area + '\\\\acf\\\\acf_sttc_trial_concat_1000ms_' + empty_suffix + '_empty_50ms_20lags_df.pkl')\n",
    "tau_sttc_trial_concat_df = calc_tau_per_unit(sttc_trial_concat_df, acf_cols, start_idx_=1)\n",
    "tau_sttc_trial_concat_df['tau_ms'] = tau_sttc_trial_concat_df['tau'] * bin_size\n",
    "tau_sttc_trial_concat_df.to_pickle(results_folder + 'non_binned\\\\' + area + '\\\\taus\\\\tau_sttc_trial_concat_1000ms_' + empty_suffix + '_empty_50ms_20lags_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42a998fa-5dd8-4dfc-ab0a-9df6222500d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_id</th>\n",
       "      <th>fit_a</th>\n",
       "      <th>fit_b</th>\n",
       "      <th>fit_c</th>\n",
       "      <th>tau</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>log_message</th>\n",
       "      <th>tau_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.719042</td>\n",
       "      <td>0.051840</td>\n",
       "      <td>-0.347076</td>\n",
       "      <td>19.290002</td>\n",
       "      <td>0.911963</td>\n",
       "      <td>ok</td>\n",
       "      <td>964.500088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ValueError</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.110746</td>\n",
       "      <td>1.005075</td>\n",
       "      <td>0.019225</td>\n",
       "      <td>0.994951</td>\n",
       "      <td>0.018581</td>\n",
       "      <td>ok</td>\n",
       "      <td>49.747526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.279456</td>\n",
       "      <td>0.164319</td>\n",
       "      <td>0.663300</td>\n",
       "      <td>6.085726</td>\n",
       "      <td>0.902339</td>\n",
       "      <td>ok</td>\n",
       "      <td>304.286291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>0.277495</td>\n",
       "      <td>-0.088259</td>\n",
       "      <td>3.603664</td>\n",
       "      <td>0.686497</td>\n",
       "      <td>ok</td>\n",
       "      <td>180.183206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>539</td>\n",
       "      <td>0.200108</td>\n",
       "      <td>0.138558</td>\n",
       "      <td>0.111026</td>\n",
       "      <td>7.217215</td>\n",
       "      <td>0.459831</td>\n",
       "      <td>ok</td>\n",
       "      <td>360.860736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>540</td>\n",
       "      <td>0.475262</td>\n",
       "      <td>0.478004</td>\n",
       "      <td>-0.143530</td>\n",
       "      <td>2.092032</td>\n",
       "      <td>0.834083</td>\n",
       "      <td>ok</td>\n",
       "      <td>104.601593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>541</td>\n",
       "      <td>0.809791</td>\n",
       "      <td>0.465480</td>\n",
       "      <td>-0.022679</td>\n",
       "      <td>2.148320</td>\n",
       "      <td>0.935643</td>\n",
       "      <td>ok</td>\n",
       "      <td>107.415989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>542</td>\n",
       "      <td>0.277750</td>\n",
       "      <td>0.337468</td>\n",
       "      <td>-0.157115</td>\n",
       "      <td>2.963246</td>\n",
       "      <td>0.653134</td>\n",
       "      <td>ok</td>\n",
       "      <td>148.162323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ValueError</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>539 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unit_id     fit_a     fit_b     fit_c        tau  r_squared log_message  \\\n",
       "0          0  0.719042  0.051840 -0.347076  19.290002   0.911963          ok   \n",
       "1          1       NaN       NaN       NaN        NaN        NaN  ValueError   \n",
       "2          2  0.110746  1.005075  0.019225   0.994951   0.018581          ok   \n",
       "3          3  0.279456  0.164319  0.663300   6.085726   0.902339          ok   \n",
       "4          4  0.194175  0.277495 -0.088259   3.603664   0.686497          ok   \n",
       "..       ...       ...       ...       ...        ...        ...         ...   \n",
       "534      539  0.200108  0.138558  0.111026   7.217215   0.459831          ok   \n",
       "535      540  0.475262  0.478004 -0.143530   2.092032   0.834083          ok   \n",
       "536      541  0.809791  0.465480 -0.022679   2.148320   0.935643          ok   \n",
       "537      542  0.277750  0.337468 -0.157115   2.963246   0.653134          ok   \n",
       "538      543       NaN       NaN       NaN        NaN        NaN  ValueError   \n",
       "\n",
       "         tau_ms  \n",
       "0    964.500088  \n",
       "1           NaN  \n",
       "2     49.747526  \n",
       "3    304.286291  \n",
       "4    180.183206  \n",
       "..          ...  \n",
       "534  360.860736  \n",
       "535  104.601593  \n",
       "536  107.415989  \n",
       "537  148.162323  \n",
       "538         NaN  \n",
       "\n",
       "[539 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_pearsonr_trial_avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccb70d25-bafe-4260-ba00-0ca69a7fba8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_id</th>\n",
       "      <th>fit_a</th>\n",
       "      <th>fit_b</th>\n",
       "      <th>fit_c</th>\n",
       "      <th>tau</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>log_message</th>\n",
       "      <th>tau_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.245144</td>\n",
       "      <td>17.262546</td>\n",
       "      <td>0.745102</td>\n",
       "      <td>0.057929</td>\n",
       "      <td>-6.817834e-09</td>\n",
       "      <td>ok</td>\n",
       "      <td>2.896444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>-0.027214</td>\n",
       "      <td>17.276665</td>\n",
       "      <td>0.748771</td>\n",
       "      <td>0.057882</td>\n",
       "      <td>-2.638583e-09</td>\n",
       "      <td>ok</td>\n",
       "      <td>2.894077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>-0.016710</td>\n",
       "      <td>16.262993</td>\n",
       "      <td>1.117895</td>\n",
       "      <td>0.061489</td>\n",
       "      <td>-5.471703e-09</td>\n",
       "      <td>ok</td>\n",
       "      <td>3.074465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>-0.045797</td>\n",
       "      <td>10.982746</td>\n",
       "      <td>0.805402</td>\n",
       "      <td>0.091052</td>\n",
       "      <td>-1.361375e-08</td>\n",
       "      <td>ok</td>\n",
       "      <td>4.552595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>0.085981</td>\n",
       "      <td>15.199560</td>\n",
       "      <td>1.133391</td>\n",
       "      <td>0.065791</td>\n",
       "      <td>-1.429648e-08</td>\n",
       "      <td>ok</td>\n",
       "      <td>3.289569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>489</td>\n",
       "      <td>-0.009598</td>\n",
       "      <td>19.899459</td>\n",
       "      <td>1.118962</td>\n",
       "      <td>0.050253</td>\n",
       "      <td>-1.235758e-10</td>\n",
       "      <td>ok</td>\n",
       "      <td>2.512631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>493</td>\n",
       "      <td>-0.012630</td>\n",
       "      <td>14.785423</td>\n",
       "      <td>5.944340</td>\n",
       "      <td>0.067634</td>\n",
       "      <td>-1.484684e-08</td>\n",
       "      <td>ok</td>\n",
       "      <td>3.381709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>503</td>\n",
       "      <td>-0.049089</td>\n",
       "      <td>20.914123</td>\n",
       "      <td>1.180155</td>\n",
       "      <td>0.047815</td>\n",
       "      <td>-1.208277e-09</td>\n",
       "      <td>ok</td>\n",
       "      <td>2.390729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>517</td>\n",
       "      <td>0.027741</td>\n",
       "      <td>20.252109</td>\n",
       "      <td>0.770251</td>\n",
       "      <td>0.049378</td>\n",
       "      <td>-1.676055e-10</td>\n",
       "      <td>ok</td>\n",
       "      <td>2.468879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>536</td>\n",
       "      <td>0.071671</td>\n",
       "      <td>17.601157</td>\n",
       "      <td>1.080282</td>\n",
       "      <td>0.056814</td>\n",
       "      <td>-4.970807e-09</td>\n",
       "      <td>ok</td>\n",
       "      <td>2.840722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unit_id     fit_a      fit_b     fit_c       tau     r_squared  \\\n",
       "11        11  0.245144  17.262546  0.745102  0.057929 -6.817834e-09   \n",
       "16        16 -0.027214  17.276665  0.748771  0.057882 -2.638583e-09   \n",
       "20        20 -0.016710  16.262993  1.117895  0.061489 -5.471703e-09   \n",
       "35        35 -0.045797  10.982746  0.805402  0.091052 -1.361375e-08   \n",
       "54        54  0.085981  15.199560  1.133391  0.065791 -1.429648e-08   \n",
       "..       ...       ...        ...       ...       ...           ...   \n",
       "484      489 -0.009598  19.899459  1.118962  0.050253 -1.235758e-10   \n",
       "488      493 -0.012630  14.785423  5.944340  0.067634 -1.484684e-08   \n",
       "498      503 -0.049089  20.914123  1.180155  0.047815 -1.208277e-09   \n",
       "512      517  0.027741  20.252109  0.770251  0.049378 -1.676055e-10   \n",
       "531      536  0.071671  17.601157  1.080282  0.056814 -4.970807e-09   \n",
       "\n",
       "    log_message    tau_ms  \n",
       "11           ok  2.896444  \n",
       "16           ok  2.894077  \n",
       "20           ok  3.074465  \n",
       "35           ok  4.552595  \n",
       "54           ok  3.289569  \n",
       "..          ...       ...  \n",
       "484          ok  2.512631  \n",
       "488          ok  3.381709  \n",
       "498          ok  2.390729  \n",
       "512          ok  2.468879  \n",
       "531          ok  2.840722  \n",
       "\n",
       "[68 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_sttc_trial_concat_df.query('r_squared < 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516d2e1-ad61-4f87-b37d-90f6e8fb62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "area = 'pfp' \n",
    "\n",
    "# pearsonr trial avg\n",
    "pearsonr_trial_avg_df = pd.read_pickle(results_folder + 'binned\\\\' + area + '\\\\acf\\\\acf_pearsonr_trial_avg_1000ms_with_empty_50ms_20lags_df.pkl')\n",
    "tau_pearsonr_trial_avg_df = calc_tau_per_unit(pearsonr_trial_avg_df, acf_cols, start_idx_=1)\n",
    "tau_pearsonr_trial_avg_df['tau_ms'] = tau_pearsonr_trial_avg_df['tau'] * bin_size\n",
    "tau_pearsonr_trial_avg_df.to_pickle(results_folder + 'binned\\\\' + area + '\\\\taus\\\\tau_pearsonr_trial_avg_1000ms_with_empty_50ms_20lags_df.pkl')\n",
    "\n",
    "# sttc trial avg\n",
    "sttc_trial_avg_df = pd.read_pickle(results_folder + 'non_binned\\\\' + area + '\\\\acf\\\\acf_sttc_trial_avg_1000ms_with_empty_50ms_20lags_df.pkl')\n",
    "tau_sttc_trial_avg_df = calc_tau_per_unit(sttc_trial_avg_df, acf_cols, start_idx_=1)\n",
    "tau_sttc_trial_avg_df['tau_ms'] = tau_sttc_trial_avg_df['tau'] * bin_size\n",
    "tau_sttc_trial_avg_df.to_pickle(results_folder + 'non_binned\\\\' + area + '\\\\taus\\\\tau_sttc_trial_avg_1000ms_with_empty_50ms_20lags_df.pkl')\n",
    "\n",
    "# sttc trial concat\n",
    "sttc_trial_concat_df = pd.read_pickle(results_folder + 'non_binned\\\\' + area + '\\\\acf\\\\acf_sttc_trial_concat_1000ms_with_empty_50ms_20lags_df.pkl')\n",
    "tau_sttc_trial_concat_df = calc_tau_per_unit(sttc_trial_concat_df, acf_cols, start_idx_=1)\n",
    "tau_sttc_trial_concat_df['tau_ms'] = tau_sttc_trial_concat_df['tau'] * bin_size\n",
    "tau_sttc_trial_concat_df.to_pickle(results_folder + 'non_binned\\\\' + area + '\\\\taus\\\\tau_sttc_trial_concat_1000ms_with_empty_50ms_20lags_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65f10f5-705f-480f-a740-5d69ae4846fd",
   "metadata": {},
   "source": [
    "#### One tau per trial\n",
    "\n",
    "3 methods of doing that:\n",
    "* Pearsonr\n",
    "* ACF formula\n",
    "* iSTTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf31bf1-1c7c-4371-be0b-b3fdb491c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lags = 20\n",
    "bin_size = 50\n",
    "acf_cols = ['acf_' + str(i) for i in range(n_lags)]\n",
    "print('acf_cols {}'.format(acf_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2e68e-c325-4737-ba33-db125f8b1bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "area = 'pfdl' \n",
    "\n",
    "# pearsonr \n",
    "# pearsonr_per_trial_df = pd.read_pickle(results_folder + 'binned\\\\' + area + '\\\\acf\\\\acf_pearsonr_per_trial_1000ms_with_empty_50ms_20lags_df.pkl')\n",
    "# print(f'N rows with NaN {pearsonr_per_trial_df.isnull().any(axis=1).sum()} out of {len(pearsonr_per_trial_df)}, dropping before tau calc ...')\n",
    "# pearsonr_per_trial_df.dropna(inplace=True)\n",
    "# pearsonr_per_trial_df.reset_index(drop=True, inplace=True)\n",
    "# tau_pearsonr_per_trial_df = calc_tau_per_unit(pearsonr_per_trial_df, acf_cols[:-1], start_idx_=1)\n",
    "# tau_pearsonr_per_trial_df['tau_ms'] = tau_pearsonr_per_trial_df['tau'] * bin_size\n",
    "# tau_pearsonr_per_trial_df.insert(1, 'trial_id', pearsonr_per_trial_df['trial_id'])\n",
    "# tau_pearsonr_per_trial_df.insert(2, 'condition_id', pearsonr_per_trial_df['condition_id'])\n",
    "# tau_pearsonr_per_trial_df.insert(3, 'spike_count', pearsonr_per_trial_df['spike_count'])\n",
    "# tau_pearsonr_per_trial_df.insert(4, 'fr_hz', pearsonr_per_trial_df['fr_hz'])\n",
    "# print(f'N rows with NaN {tau_pearsonr_per_trial_df.isnull().any(axis=1).sum()} out of {len(tau_pearsonr_per_trial_df)}')\n",
    "# tau_pearsonr_per_trial_df.to_pickle(results_folder + 'binned\\\\' + area + '\\\\taus\\\\tau_pearsonr_per_trial_1000ms_with_empty_50ms_20lags_df.pkl')\n",
    "\n",
    "# # acf formula\n",
    "# pcf_proper_per_trial_df = pd.read_pickle(results_folder + 'binned\\\\' + area + '\\\\acf\\\\acf_proper_per_trial_1000ms_with_empty_50ms_20lags_df.pkl')\n",
    "# print(f'N rows with NaN {pcf_proper_per_trial_df.isnull().any(axis=1).sum()} out of {len(pcf_proper_per_trial_df)}, dropping before tau calc ...')\n",
    "# pcf_proper_per_trial_df.dropna(inplace=True)\n",
    "# pcf_proper_per_trial_df.reset_index(drop=True, inplace=True)\n",
    "# tau_acf_proper_per_trial_df = calc_tau_per_unit(pcf_proper_per_trial_df, acf_cols, start_idx_=1)\n",
    "# tau_acf_proper_per_trial_df['tau_ms'] = tau_acf_proper_per_trial_df['tau'] * bin_size\n",
    "# tau_acf_proper_per_trial_df.insert(1, 'trial_id', pcf_proper_per_trial_df['trial_id'])\n",
    "# tau_acf_proper_per_trial_df.insert(2, 'condition_id', pcf_proper_per_trial_df['condition_id'])\n",
    "# tau_acf_proper_per_trial_df.insert(3, 'spike_count', pcf_proper_per_trial_df['spike_count'])\n",
    "# tau_acf_proper_per_trial_df.insert(4, 'fr_hz', pcf_proper_per_trial_df['fr_hz'])\n",
    "# print(f'N rows with NaN {tau_acf_proper_per_trial_df.isnull().any(axis=1).sum()} out of {len(tau_acf_proper_per_trial_df)}')\n",
    "# tau_acf_proper_per_trial_df.to_pickle(results_folder + 'binned\\\\' + area + '\\\\taus\\\\tau_acf_proper_per_trial_1000ms_with_empty_50ms_20lags_df.pkl')\n",
    "\n",
    "# isttc\n",
    "sttc_per_trial_df = pd.read_pickle(results_folder + 'non_binned\\\\' + area + '\\\\acf\\\\acf_isttc_per_trial_1000ms_with_empty_50ms_20lags_df.pkl')\n",
    "print(f'N rows with NaN {sttc_per_trial_df.isnull().any(axis=1).sum()} out of {len(sttc_per_trial_df)}, dropping before tau calc ...')\n",
    "sttc_per_trial_df.dropna(inplace=True)\n",
    "sttc_per_trial_df.reset_index(drop=True, inplace=True)\n",
    "tau_isttc_per_trial_df = calc_tau_per_unit(sttc_per_trial_df, acf_cols, start_idx_=1)\n",
    "tau_isttc_per_trial_df['tau_ms'] = tau_isttc_per_trial_df['tau'] * bin_size\n",
    "tau_isttc_per_trial_df.insert(1, 'trial_id', sttc_per_trial_df['trial_id'])\n",
    "tau_isttc_per_trial_df.insert(2, 'condition_id', sttc_per_trial_df['condition_id'])\n",
    "tau_isttc_per_trial_df.insert(3, 'spike_count', sttc_per_trial_df['spike_count'])\n",
    "tau_isttc_per_trial_df.insert(4, 'fr_hz', sttc_per_trial_df['fr_hz'])\n",
    "print(f'N rows with NaN {tau_isttc_per_trial_df.isnull().any(axis=1).sum()} out of {len(tau_isttc_per_trial_df)}')\n",
    "tau_isttc_per_trial_df.to_pickle(results_folder + 'non_binned\\\\' + area + '\\\\taus\\\\tau_isttc_per_trial_1000ms_with_empty_50ms_20lags_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2bc29-5a5c-42cb-9cac-14b8e3cd8b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_acf_proper_per_trial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d3e05-bea0-4f7b-83f9-da6839866c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr_per_trial_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f85d5-e9b7-487b-a820-45213b252421",
   "metadata": {},
   "source": [
    "### todo (metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f066b566-b9cb-4edf-8257-7c9f7d95b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows_with_nans = pearsonr_trial_avg_df[acf_cols].isnull().any(axis=1).sum()\n",
    "n_rows_with_nans_perc = n_rows_with_nans / len(pearsonr_trial_avg_df) * 100\n",
    "# acf_df.dropna(inplace=True)\n",
    "\n",
    "n_rows_with_nans_tau = tau_pearsonr_trial_avg_df['tau_ms'].isnull().sum()\n",
    "n_rows_with_nans_tau_perc = n_rows_with_nans_tau / len(tau_pearsonr_trial_avg_df) * 100\n",
    "# tau_df.dropna(inplace=True)\n",
    "\n",
    "print('acf n_rows_with_nans_perc {}, tau n_rows_with_nans_tau_perc {}'.format(n_rows_with_nans_perc, n_rows_with_nans_tau_perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10482216-9577-43a6-ae86-f6f4dd9be4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows_with_nans = sttc_trial_avg_df[acf_cols].isnull().any(axis=1).sum()\n",
    "n_rows_with_nans_perc = n_rows_with_nans / len(sttc_trial_avg_df) * 100\n",
    "# acf_df.dropna(inplace=True)\n",
    "\n",
    "n_rows_with_nans_tau = tau_sttc_trial_avg_df['tau_ms'].isnull().sum()\n",
    "n_rows_with_nans_tau_perc = n_rows_with_nans_tau / len(tau_sttc_trial_avg_df) * 100\n",
    "# tau_df.dropna(inplace=True)\n",
    "\n",
    "print('acf n_rows_with_nans_perc {}, tau n_rows_with_nans_tau_perc {}'.format(n_rows_with_nans_perc, n_rows_with_nans_tau_perc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
